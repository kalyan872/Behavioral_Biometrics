{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "18WrWf2U2Mbaw6rejp80uVYtkxPc4nNT6",
      "authorship_tag": "ABX9TyMX/Rt6OTdUTfDOBhm4H1gR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steeve2412/IP2Project/blob/Custom_Dataset/Custom_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6Dqqjror28i",
        "outputId": "1819d841-adf6-4477-c0f8-217b9b81acc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/KeyStroke11july.csv')\n",
        "\n",
        "# Extract relevant features\n",
        "features = data[['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations']]\n",
        "target = data['Email']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to preprocess features\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def preprocess_features(data):\n",
        "    # Convert string representation of lists to actual lists\n",
        "    data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "    data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(literal_eval)\n",
        "\n",
        "    # Find the maximum length among the lists in each column\n",
        "    max_len_hold_times = max(data['Hold Times'].apply(len))\n",
        "    max_len_flight_times = max(data['Flight Times'].apply(len))\n",
        "    max_len_press_release = max(data['Press/Release Timings'].apply(len))\n",
        "    max_len_key_combinations = max(data['Key Combinations'].apply(len))\n",
        "\n",
        "    # Pad the lists with zeros to make them of equal length\n",
        "    data['Hold Times'] = data['Hold Times'].apply(lambda x: x + [0] * (max_len_hold_times - len(x)))\n",
        "    data['Flight Times'] = data['Flight Times'].apply(lambda x: x + [0] * (max_len_flight_times - len(x)))\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(lambda x: x + [(0, 0)] * (max_len_press_release - len(x)))\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(lambda x: x + [('0', '0')] * (max_len_key_combinations - len(x)))\n",
        "\n",
        "    # Convert the lists to arrays\n",
        "    data['Hold Times'] = data['Hold Times'].apply(np.array)\n",
        "    data['Flight Times'] = data['Flight Times'].apply(np.array)\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(np.array)\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(np.array)\n",
        "\n",
        "    # Normalize the Hold Times and Flight Times using MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    data['Hold Times'] = scaler.fit_transform(data['Hold Times'].tolist())\n",
        "    data['Flight Times'] = scaler.fit_transform(data['Flight Times'].tolist())\n",
        "\n",
        "    # Return preprocessed features\n",
        "    return data\n",
        "\n",
        "# Apply preprocessing to training and testing features\n",
        "X_train_preprocessed = preprocess_features(X_train)\n",
        "X_test_preprocessed = preprocess_features(X_test)\n"
      ],
      "metadata": {
        "id": "X1NeQm1YsVoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/keystrokes (1).csv')\n",
        "\n",
        "# Extract relevant features\n",
        "features = data[['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations']]\n",
        "target = data['Email']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to preprocess features\n",
        "def preprocess_features(data):\n",
        "    # Convert string representation of lists to actual lists\n",
        "    data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "    data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(literal_eval)\n",
        "\n",
        "    # Find the maximum length among the lists in each column\n",
        "    max_len_hold_times = max(data['Hold Times'].apply(len))\n",
        "    max_len_flight_times = max(data['Flight Times'].apply(len))\n",
        "    max_len_press_release = max(data['Press/Release Timings'].apply(len))\n",
        "    max_len_key_combinations = max(data['Key Combinations'].apply(len))\n",
        "\n",
        "    # Pad the lists with zeros to make them of equal length\n",
        "    data['Hold Times'] = data['Hold Times'].apply(lambda x: x + [0] * (max_len_hold_times - len(x)))\n",
        "    data['Flight Times'] = data['Flight Times'].apply(lambda x: x + [0] * (max_len_flight_times - len(x)))\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(lambda x: x + [(0, 0)] * (max_len_press_release - len(x)))\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(lambda x: x + [('0', '0')] * (max_len_key_combinations - len(x)))\n",
        "\n",
        "    # Convert the lists to arrays\n",
        "    data['Hold Times'] = data['Hold Times'].apply(np.array)\n",
        "    data['Flight Times'] = data['Flight Times'].apply(np.array)\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(np.array)\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(np.array)\n",
        "\n",
        "    # Normalize the Hold Times and Flight Times using MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    data['Hold Times'] = scaler.fit_transform(data['Hold Times'].tolist())\n",
        "    data['Flight Times'] = scaler.fit_transform(data['Flight Times'].tolist())\n",
        "\n",
        "    # Return preprocessed features\n",
        "    return data\n",
        "\n",
        "# Flatten the Hold Times column\n",
        "X_train_preprocessed['Hold Times'] = X_train_preprocessed['Hold Times'].apply(lambda x: x.flatten())\n",
        "X_test_preprocessed['Hold Times'] = X_test_preprocessed['Hold Times'].apply(lambda x: x.flatten())\n",
        "\n",
        "# Verify the shape of the Hold Times column\n",
        "print(X_train_preprocessed['Hold Times'].shape)\n",
        "print(X_test_preprocessed['Hold Times'].shape)\n",
        "\n",
        "# Reshape the input shape for the MLP model\n",
        "input_shape = (X_train_preprocessed['Hold Times'].shape[1],)\n",
        "\n",
        "# Create the MLP model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=input_shape),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "MjBBAGWy1ZxG",
        "outputId": "f3f1edf9-c4c4-4160-cdef-c1699589da6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f7fd4a8852f9>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Flatten the Hold Times column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mX_train_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mX_test_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[0;32m-> 4771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4773\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-f7fd4a8852f9>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Flatten the Hold Times column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mX_train_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mX_test_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'flatten'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VhV05fzzdHMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zr36fcdJdHzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load and preprocess the data\n",
        "data = pd.read_csv('/content/drive/MyDrive/keystrokes (1).csv')\n",
        "\n",
        "# Define the max sequence lengths\n",
        "max_len_hold_times = max(data['Hold Times'].apply(len))\n",
        "max_len_flight_times = max(data['Flight Times'].apply(len))\n",
        "max_len_press_release = max(data['Press/Release Timings'].apply(len))\n",
        "max_len_key_combinations = max(data['Key Combinations'].apply(len))\n",
        "\n",
        "# Convert the string representations to lists\n",
        "data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "data['Key Combinations'] = data['Key Combinations'].apply(literal_eval)\n",
        "\n",
        "# Pad the sequences with zeros\n",
        "data['Hold Times'] = data['Hold Times'].apply(lambda x: x + [0] * (max_len_hold_times - len(x)))\n",
        "data['Flight Times'] = data['Flight Times'].apply(lambda x: x + [0] * (max_len_flight_times - len(x)))\n",
        "data['Press/Release Timings'] = data['Press/Release Timings'].apply(lambda x: x + [(0, 0)] * (max_len_press_release - len(x)))\n",
        "data['Key Combinations'] = data['Key Combinations'].apply(lambda x: x + [('0', '0')] * (max_len_key_combinations - len(x)))\n",
        "\n",
        "# Convert the sequences to numpy arrays\n",
        "data['Hold Times'] = data['Hold Times'].apply(np.array)\n",
        "data['Flight Times'] = data['Flight Times'].apply(np.array)\n",
        "data['Press/Release Timings'] = data['Press/Release Timings'].apply(np.array)\n",
        "data['Key Combinations'] = data['Key Combinations'].apply(np.array)\n",
        "\n",
        "# Scale the numerical features\n",
        "scaler = MinMaxScaler()\n",
        "data['Hold Times'] = scaler.fit_transform(data['Hold Times'].tolist())\n",
        "data['Flight Times'] = scaler.fit_transform(data['Flight Times'].tolist())\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "data['Labels'] = label_encoder.fit_transform(data['Labels'])\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations']], data['Labels'], test_size=0.2, random_state=42)\n",
        "\n",
        "# One-hot encode the target labels\n",
        "num_classes = len(label_encoder.classes_)\n",
        "y_train_encoded = pd.get_dummies(y_train).values\n",
        "y_test_encoded = pd.get_dummies(y_test).values\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(max_len_hold_times,)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train['Hold Times'], y_train_encoded, epochs=10, validation_data=(X_test['Hold Times'], y_test_encoded))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test['Hold Times'], y_test_encoded)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Make predictions on new data\n",
        "new_data = preprocess_features(new_data)  # Preprocess the new data\n",
        "predictions = model.predict(new_data['Hold Times'])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "8rEd52uH0GcM",
        "outputId": "b09c94b5-c0e4-4943-e21f-b3e9bca264ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Labels'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-11e1fc747e4b>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Encode the target labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Split the data into train and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/keystrokes (1).csv')\n",
        "\n",
        "# Extract relevant features\n",
        "features = data[['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations']]\n",
        "target = data['Email']\n",
        "\n",
        "# Function to preprocess features\n",
        "def preprocess_features(data):\n",
        "    # Convert string representation of lists to actual lists\n",
        "    data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "    data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(literal_eval)\n",
        "\n",
        "    # Pad the lists with zeros to make them of equal length\n",
        "    max_len_hold_times = max(data['Hold Times'].apply(len))\n",
        "    max_len_flight_times = max(data['Flight Times'].apply(len))\n",
        "    max_len_press_release = max(data['Press/Release Timings'].apply(len))\n",
        "    max_len_key_combinations = max(data['Key Combinations'].apply(len))\n",
        "\n",
        "    data['Hold Times'] = data['Hold Times'].apply(lambda x: x + [0] * (max_len_hold_times - len(x)))\n",
        "    data['Flight Times'] = data['Flight Times'].apply(lambda x: x + [0] * (max_len_flight_times - len(x)))\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(lambda x: x + [(0, 0)] * (max_len_press_release - len(x)))\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(lambda x: x + [('0', '0')] * (max_len_key_combinations - len(x)))\n",
        "\n",
        "    # Convert the lists to arrays\n",
        "    data['Hold Times'] = data['Hold Times'].apply(np.array)\n",
        "    data['Flight Times'] = data['Flight Times'].apply(np.array)\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(np.array)\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(np.array)\n",
        "\n",
        "    # Normalize the Hold Times and Flight Times using MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    data['Hold Times'] = scaler.fit_transform(data['Hold Times'].tolist())\n",
        "    data['Flight Times'] = scaler.fit_transform(data['Flight Times'].tolist())\n",
        "\n",
        "    return data\n",
        "\n",
        "# Preprocess features\n",
        "preprocessed_data = preprocess_features(features)\n",
        "\n",
        "# Split the preprocessed data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the target values to numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Create the MLP model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(1,)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train['Hold Times'], y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test['Hold Times'], y_test_encoded))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test['Hold Times'], y_test_encoded)\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n",
        "\n",
        "# Make predictions on new data\n",
        "new_data1 = pd.read_csv('/content/drive/MyDrive/keystrokes (1).csv')\n",
        "new_data = preprocess_features(new_data1)  # Preprocess the new data\n",
        "predictions = model.predict(new_data['Hold Times'])\n",
        "\n",
        "# Convert the predicted values back to categorical labels\n",
        "predicted_labels = label_encoder.inverse_transform(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hY5dNgpxdKEN",
        "outputId": "0ddd54bd-a0b3-4835-ee2a-b21a399091a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-236429e5ebbc>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
            "<ipython-input-9-236429e5ebbc>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
            "<ipython-input-9-236429e5ebbc>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
            "<ipython-input-9-236429e5ebbc>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Key Combinations'] = data['Key Combinations'].apply(literal_eval)\n",
            "<ipython-input-9-236429e5ebbc>:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Hold Times'] = data['Hold Times'].apply(lambda x: x + [0] * (max_len_hold_times - len(x)))\n",
            "<ipython-input-9-236429e5ebbc>:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Flight Times'] = data['Flight Times'].apply(lambda x: x + [0] * (max_len_flight_times - len(x)))\n",
            "<ipython-input-9-236429e5ebbc>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Press/Release Timings'] = data['Press/Release Timings'].apply(lambda x: x + [(0, 0)] * (max_len_press_release - len(x)))\n",
            "<ipython-input-9-236429e5ebbc>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Key Combinations'] = data['Key Combinations'].apply(lambda x: x + [('0', '0')] * (max_len_key_combinations - len(x)))\n",
            "<ipython-input-9-236429e5ebbc>:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Hold Times'] = data['Hold Times'].apply(np.array)\n",
            "<ipython-input-9-236429e5ebbc>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Flight Times'] = data['Flight Times'].apply(np.array)\n",
            "<ipython-input-9-236429e5ebbc>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Press/Release Timings'] = data['Press/Release Timings'].apply(np.array)\n",
            "<ipython-input-9-236429e5ebbc>:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Key Combinations'] = data['Key Combinations'].apply(np.array)\n",
            "<ipython-input-9-236429e5ebbc>:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Hold Times'] = scaler.fit_transform(data['Hold Times'].tolist())\n",
            "<ipython-input-9-236429e5ebbc>:46: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Flight Times'] = scaler.fit_transform(data['Flight Times'].tolist())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "6/6 [==============================] - 1s 35ms/step - loss: 0.5861 - accuracy: 0.0343 - val_loss: 0.3236 - val_accuracy: 0.0455\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1606 - accuracy: 0.0514 - val_loss: -0.1241 - val_accuracy: 0.0455\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: -0.3156 - accuracy: 0.0514 - val_loss: -0.6386 - val_accuracy: 0.0455\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: -0.8697 - accuracy: 0.0514 - val_loss: -1.2242 - val_accuracy: 0.0455\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: -1.4824 - accuracy: 0.0514 - val_loss: -1.9030 - val_accuracy: 0.0455\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: -2.2230 - accuracy: 0.0514 - val_loss: -2.6835 - val_accuracy: 0.0455\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 0s 9ms/step - loss: -3.0702 - accuracy: 0.0514 - val_loss: -3.5957 - val_accuracy: 0.0455\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: -4.0632 - accuracy: 0.0514 - val_loss: -4.6558 - val_accuracy: 0.0455\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: -5.1851 - accuracy: 0.0514 - val_loss: -5.8744 - val_accuracy: 0.0455\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: -6.4709 - accuracy: 0.0514 - val_loss: -7.2605 - val_accuracy: 0.0455\n",
            "2/2 [==============================] - 0s 8ms/step - loss: -7.2605 - accuracy: 0.0455\n",
            "Test Loss: -7.260530471801758\n",
            "Test Accuracy: 0.04545454680919647\n",
            "7/7 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-236429e5ebbc>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# Convert the predicted values back to categorical labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y contains previously unseen labels: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: [0.6847092  0.6847213  0.68504757 0.6851684  0.6853254  0.6853495\n 0.6853616  0.68538576 0.68539786 0.6854341  0.68544614 0.6854944\n 0.6855065  0.68551856 0.6855427  0.6855548  0.68556684 0.68557894\n 0.685591   0.6856031  0.6856152  0.6856272  0.68563926 0.6856634\n 0.6856755  0.68568754 0.6856996  0.6857117  0.6857358  0.68574786\n 0.68575996 0.685772   0.6857841  0.68579614 0.6857962  0.68582034\n 0.6858324  0.6858444  0.6858565  0.68588066 0.6858927  0.68590474\n 0.6859168  0.6859289  0.6859409  0.685953   0.68596506 0.6859771\n 0.6859892  0.6860254  0.6860374  0.68606156 0.6860736  0.6860857\n 0.6861218  0.6861339  0.68614596 0.6861701  0.68618214 0.6862063\n 0.6862183  0.68623036 0.6862545  0.68626654 0.68627864 0.6862907\n 0.68631476 0.6863268  0.6863389  0.68636304 0.6863751  0.6864112\n 0.68642324 0.6864595  0.68647146 0.68649554 0.68650764 0.6865197\n 0.6865317  0.68654376 0.68656784 0.686604   0.6866402  0.68665224\n 0.68674856 0.6867968  0.6868329  0.6869895  0.6870497  0.68707377\n 0.68709785 0.6872062  0.68777895 0.6879504  0.6907468  0.6908459\n 0.69093275 0.87357485]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/keystrokes (1).csv')\n",
        "\n",
        "# Separate the features and target variable\n",
        "features = data.drop(columns=['ID', 'Password', 'Email', 'Total Hold Time', 'Total Flight Time', 'Total Key Combinations'])\n",
        "target = data['Target']\n",
        "\n",
        "# Preprocess features\n",
        "def preprocess_features(features):\n",
        "    # Convert categorical variables to numeric using label encoding\n",
        "    categorical_cols = ['Hold Times', 'Press/Release Timings', 'Key Combinations']\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        features[col] = le.fit_transform(features[col])\n",
        "\n",
        "    return features\n",
        "\n",
        "preprocessed_data = preprocess_features(features)\n",
        "\n",
        "# Split the preprocessed data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the target variable to binary representation\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test, y_test_encoded))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "rK3JBxnQg2rf",
        "outputId": "cf45d842-e41b-4698-b840-4ae2418d5f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Target'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-44d0fc1cf5d5>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Separate the features and target variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Password'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Email'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Total Hold Time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Total Flight Time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Total Key Combinations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Preprocess features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Target'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/keystrokes (1).csv')\n",
        "\n",
        "# Convert string representations of lists to actual lists\n",
        "data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "data['Key Combinations'] = data['Key Combinations'].apply(literal_eval)\n",
        "\n",
        "# Calculate the maximum lengths for padding\n",
        "max_len_hold_times = max(data['Hold Times'].apply(len))\n",
        "max_len_flight_times = max(data['Flight Times'].apply(len))\n",
        "max_len_press_release = max(data['Press/Release Timings'].apply(len))\n",
        "max_len_key_combinations = max(data['Key Combinations'].apply(len))\n",
        "\n",
        "# Pad the sequences with zeros to make them the same length\n",
        "data['Hold Times'] = data['Hold Times'].apply(lambda x: x + [0] * (max_len_hold_times - len(x)))\n",
        "data['Flight Times'] = data['Flight Times'].apply(lambda x: x + [0] * (max_len_flight_times - len(x)))\n",
        "data['Press/Release Timings'] = data['Press/Release Timings'].apply(lambda x: x + [(0, 0)] * (max_len_press_release - len(x)))\n",
        "data['Key Combinations'] = data['Key Combinations'].apply(lambda x: x + [('0', '0')] * (max_len_key_combinations - len(x)))\n",
        "\n",
        "# Convert the sequences to arrays\n",
        "data['Hold Times'] = data['Hold Times'].apply(np.array)\n",
        "data['Flight Times'] = data['Flight Times'].apply(np.array)\n",
        "data['Press/Release Timings'] = data['Press/Release Timings'].apply(np.array)\n",
        "data['Key Combinations'] = data['Key Combinations'].apply(np.array)\n",
        "\n",
        "# Perform min-max scaling on the time-related features\n",
        "scaler = MinMaxScaler()\n",
        "data['Hold Times'] = scaler.fit_transform(data['Hold Times'].tolist())\n",
        "data['Flight Times'] = scaler.fit_transform(data['Flight Times'].tolist())\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "data['Email'] = label_encoder.fit_transform(data['Email'])\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X = np.array(data['Hold Times'].tolist())\n",
        "X = np.expand_dims(X, axis=-1)\n",
        "y = data['Email'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the lookback and reshape the input data\n",
        "# Define the lookback and reshape the input data\n",
        "lookback = 10\n",
        "num_features = 1  # Change to 1 since you have a single feature\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], lookback, num_features))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], lookback, num_features))\n",
        "\n",
        "# Convert the labels to categorical\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# Create the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(lookback, num_features)))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "# Make predictions on a new sample\n",
        "new_sample = pd.DataFrame({\n",
        "    'Hold Times': [[0.1, 0.2, 0.3, 0.4]],\n",
        "    'Flight Times': [[0.2, 0.4, 0.6, 0.8]],\n",
        "    'Press/Release Timings': [[(0.1, 0.2), (0.3, 0.4), (0.5, 0.6)]],\n",
        "    'Key Combinations': [[('A', 'B'), ('C', 'D'), ('E', 'F')]]\n",
        "})\n",
        "new_sample['Hold Times'] = new_sample['Hold Times'].apply(np.array)\n",
        "new_sample['Flight Times'] = new_sample['Flight Times'].apply(np.array)\n",
        "new_sample['Press/Release Timings'] = new_sample['Press/Release Timings'].apply(np.array)\n",
        "new_sample['Key Combinations'] = new_sample['Key Combinations'].apply(np.array)\n",
        "new_sample['Hold Times'] = scaler.transform(new_sample['Hold Times'].tolist())\n",
        "new_sample['Flight Times'] = scaler.transform(new_sample['Flight Times'].tolist())\n",
        "new_sample = np.reshape(new_sample.values, (new_sample.shape[0], lookback, num_features))\n",
        "prediction = model.predict(new_sample)[0]\n",
        "predicted_email = label_encoder.inverse_transform([np.argmax(prediction)])[0]\n",
        "print('Predicted email:', predicted_email)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "Hd8OW0asypx0",
        "outputId": "257b4c11-9584-43a6-fc93-12091a199b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-f1f9de853801>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mlookback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Change to 1 since you have a single feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    296\u001b[0m            [5, 6]])\n\u001b[1;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 175 into shape (175,10,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/keystrokes (1).csv')\n",
        "\n",
        "# Extract relevant features\n",
        "features = data[['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations']]\n",
        "target = data['Email']\n",
        "\n",
        "# Function to preprocess features\n",
        "def preprocess_features(data):\n",
        "    # Convert string representation of lists to actual lists\n",
        "    data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "    data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(literal_eval)\n",
        "\n",
        "    # Pad the lists with zeros to make them of equal length\n",
        "    max_len_hold_times = max(data['Hold Times'].apply(len))\n",
        "    max_len_flight_times = max(data['Flight Times'].apply(len))\n",
        "    max_len_press_release = max(data['Press/Release Timings'].apply(len))\n",
        "    max_len_key_combinations = max(data['Key Combinations'].apply(len))\n",
        "\n",
        "    data['Hold Times'] = data['Hold Times'].apply(lambda x: x + [0] * (max_len_hold_times - len(x)))\n",
        "    data['Flight Times'] = data['Flight Times'].apply(lambda x: x + [0] * (max_len_flight_times - len(x)))\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(lambda x: x + [(0, 0)] * (max_len_press_release - len(x)))\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(lambda x: x + [('0', '0')] * (max_len_key_combinations - len(x)))\n",
        "\n",
        "    # Convert the lists to arrays\n",
        "    data['Hold Times'] = data['Hold Times'].apply(np.array)\n",
        "    data['Flight Times'] = data['Flight Times'].apply(np.array)\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(np.array)\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(np.array)\n",
        "\n",
        "    # Normalize the Hold Times and Flight Times using MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    data['Hold Times'] = scaler.fit_transform(data['Hold Times'].tolist())\n",
        "    data['Flight Times'] = scaler.fit_transform(data['Flight Times'].tolist())\n",
        "\n",
        "    return data\n",
        "\n",
        "# Preprocess features\n",
        "preprocessed_data = preprocess_features(features)\n",
        "\n",
        "# Split the preprocessed data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the target values to numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Reshape the input data for SCNN (Conv1D expects a 3D input)\n",
        "X_train_reshaped = np.array(X_train['Hold Times'].tolist()).reshape(-1, X_train['Hold Times'].shape[1], 1)\n",
        "X_test_reshaped = np.array(X_test['Hold Times'].tolist()).reshape(-1, X_test['Hold Times'].shape[1], 1)\n",
        "\n",
        "# Create the SCNN model\n",
        "model = Sequential([\n",
        "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_reshaped, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test_reshaped, y_test_encoded))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test_encoded)\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n",
        "\n",
        "# # Make predictions on new data\n",
        "# new_data1 = pd.read_csv('/content/drive/MyDrive/keystrokes (1).csv')\n",
        "# new_data = preprocess_features(new_data1)  # Preprocess the new data\n",
        "# new_data_reshaped = np.array(new_data['Hold Times'].tolist()).reshape(-1, new_data['Hold Times'].shape[1], 1)\n",
        "# predictions = model.predict(new_data_reshaped)\n",
        "\n",
        "# # Convert the predicted values back to categorical labels\n",
        "# predicted_labels = label_encoder.inverse_transform(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bQOFkPeSSLsR",
        "outputId": "70975ed3-29bf-4453-a810-e0a93283c4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d388c86c5b40>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
            "<ipython-input-2-d388c86c5b40>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
            "<ipython-input-2-d388c86c5b40>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
            "<ipython-input-2-d388c86c5b40>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Key Combinations'] = data['Key Combinations'].apply(literal_eval)\n",
            "<ipython-input-2-d388c86c5b40>:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Hold Times'] = data['Hold Times'].apply(lambda x: x + [0] * (max_len_hold_times - len(x)))\n",
            "<ipython-input-2-d388c86c5b40>:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Flight Times'] = data['Flight Times'].apply(lambda x: x + [0] * (max_len_flight_times - len(x)))\n",
            "<ipython-input-2-d388c86c5b40>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Press/Release Timings'] = data['Press/Release Timings'].apply(lambda x: x + [(0, 0)] * (max_len_press_release - len(x)))\n",
            "<ipython-input-2-d388c86c5b40>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Key Combinations'] = data['Key Combinations'].apply(lambda x: x + [('0', '0')] * (max_len_key_combinations - len(x)))\n",
            "<ipython-input-2-d388c86c5b40>:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Hold Times'] = data['Hold Times'].apply(np.array)\n",
            "<ipython-input-2-d388c86c5b40>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Flight Times'] = data['Flight Times'].apply(np.array)\n",
            "<ipython-input-2-d388c86c5b40>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Press/Release Timings'] = data['Press/Release Timings'].apply(np.array)\n",
            "<ipython-input-2-d388c86c5b40>:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Key Combinations'] = data['Key Combinations'].apply(np.array)\n",
            "<ipython-input-2-d388c86c5b40>:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Hold Times'] = scaler.fit_transform(data['Hold Times'].tolist())\n",
            "<ipython-input-2-d388c86c5b40>:46: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Flight Times'] = scaler.fit_transform(data['Flight Times'].tolist())\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d388c86c5b40>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Reshape the input data for SCNN (Conv1D expects a 3D input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mX_train_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mX_test_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/keystrokes (1).csv')\n",
        "\n",
        "# Extract relevant features\n",
        "features = data[['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations']]\n",
        "target = data['Email']\n",
        "\n",
        "# Function to preprocess features\n",
        "def preprocess_features(data):\n",
        "    # Convert string representation of lists to actual lists\n",
        "    data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "    data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(literal_eval)\n",
        "\n",
        "    # Pad the lists with zeros to make them of equal length\n",
        "    max_len_hold_times = max(data['Hold Times'].apply(len))\n",
        "    max_len_flight_times = max(data['Flight Times'].apply(len))\n",
        "    max_len_press_release = max(data['Press/Release Timings'].apply(len))\n",
        "    max_len_key_combinations = max(data['Key Combinations'].apply(len))\n",
        "\n",
        "    data['Hold Times'] = data['Hold Times'].apply(lambda x: x + [0] * (max_len_hold_times - len(x)))\n",
        "    data['Flight Times'] = data['Flight Times'].apply(lambda x: x + [0] * (max_len_flight_times - len(x)))\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(lambda x: x + [(0, 0)] * (max_len_press_release - len(x)))\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(lambda x: x + [('0', '0')] * (max_len_key_combinations - len(x)))\n",
        "\n",
        "    # Convert the lists to arrays\n",
        "    data['Hold Times'] = data['Hold Times'].apply(np.array)\n",
        "    data['Flight Times'] = data['Flight Times'].apply(np.array)\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(np.array)\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(np.array)\n",
        "\n",
        "    # Normalize the Hold Times and Flight Times using MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    data['Hold Times'] = data['Hold Times'].apply(lambda x: scaler.fit_transform(x.reshape(-1, 1)).flatten())\n",
        "    data['Flight Times'] = data['Flight Times'].apply(lambda x: scaler.fit_transform(x.reshape(-1, 1)).flatten())\n",
        "\n",
        "    return data\n",
        "\n",
        "# Preprocess features\n",
        "preprocessed_data = preprocess_features(features)\n",
        "\n",
        "# Split the preprocessed data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the input data for SCNN (Conv1D expects a 3D input)\n",
        "X_train_reshaped = np.stack(X_train['Hold Times'].values).reshape(-1, X_train['Hold Times'].values.shape[1], 1)\n",
        "X_test_reshaped = np.stack(X_test['Hold Times'].values).reshape(-1, X_train['Hold Times'].values.shape[1], 1)\n",
        "\n",
        "# Convert the target labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Create the SCNN model\n",
        "model = Sequential([\n",
        "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_reshaped, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test_reshaped, y_test_encoded))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test_encoded)\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3n2R1iDKS05j",
        "outputId": "364576cb-6abe-42fc-9ab8-7ecd96789fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-ee59e290b988>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
            "<ipython-input-23-ee59e290b988>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
            "<ipython-input-23-ee59e290b988>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
            "<ipython-input-23-ee59e290b988>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Key Combinations'] = data['Key Combinations'].apply(literal_eval)\n",
            "<ipython-input-23-ee59e290b988>:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Hold Times'] = data['Hold Times'].apply(lambda x: x + [0] * (max_len_hold_times - len(x)))\n",
            "<ipython-input-23-ee59e290b988>:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Flight Times'] = data['Flight Times'].apply(lambda x: x + [0] * (max_len_flight_times - len(x)))\n",
            "<ipython-input-23-ee59e290b988>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Press/Release Timings'] = data['Press/Release Timings'].apply(lambda x: x + [(0, 0)] * (max_len_press_release - len(x)))\n",
            "<ipython-input-23-ee59e290b988>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Key Combinations'] = data['Key Combinations'].apply(lambda x: x + [('0', '0')] * (max_len_key_combinations - len(x)))\n",
            "<ipython-input-23-ee59e290b988>:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Hold Times'] = data['Hold Times'].apply(np.array)\n",
            "<ipython-input-23-ee59e290b988>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Flight Times'] = data['Flight Times'].apply(np.array)\n",
            "<ipython-input-23-ee59e290b988>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Press/Release Timings'] = data['Press/Release Timings'].apply(np.array)\n",
            "<ipython-input-23-ee59e290b988>:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Key Combinations'] = data['Key Combinations'].apply(np.array)\n",
            "<ipython-input-23-ee59e290b988>:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Hold Times'] = data['Hold Times'].apply(lambda x: scaler.fit_transform(x.reshape(-1, 1)).flatten())\n",
            "<ipython-input-23-ee59e290b988>:46: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['Flight Times'] = data['Flight Times'].apply(lambda x: scaler.fit_transform(x.reshape(-1, 1)).flatten())\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ee59e290b988>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Reshape the input data for SCNN (Conv1D expects a 3D input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mX_train_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mX_test_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/custom_data.csv')\n",
        "\n",
        "# Extract relevant features\n",
        "features = data[['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations']]\n",
        "target = data['Email']\n",
        "\n",
        "# Function to preprocess features\n",
        "def preprocess_features(data):\n",
        "    # Make a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "    data = data.copy()\n",
        "\n",
        "    # Convert string representation of lists to actual lists\n",
        "    data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "    data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(literal_eval)\n",
        "\n",
        "    # Pad the lists with zeros to make them of equal length\n",
        "    max_len_hold_times = max(data['Hold Times'].apply(len))\n",
        "    max_len_flight_times = max(data['Flight Times'].apply(len))\n",
        "    max_len_press_release = max(data['Press/Release Timings'].apply(len))\n",
        "    max_len_key_combinations = max(data['Key Combinations'].apply(len))\n",
        "\n",
        "    data['Hold Times'] = data['Hold Times'].apply(lambda x: x + [0] * (max_len_hold_times - len(x)))\n",
        "    data['Flight Times'] = data['Flight Times'].apply(lambda x: x + [0] * (max_len_flight_times - len(x)))\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(lambda x: x + [(0, 0)] * (max_len_press_release - len(x)))\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(lambda x: x + [('0', '0')] * (max_len_key_combinations - len(x)))\n",
        "\n",
        "    # Convert the lists to arrays\n",
        "    data['Hold Times'] = data['Hold Times'].apply(np.array)\n",
        "    data['Flight Times'] = data['Flight Times'].apply(np.array)\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(np.array)\n",
        "    data['Key Combinations'] = data['Key Combinations'].apply(np.array)\n",
        "\n",
        "    # Normalize the Hold Times and Flight Times using MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    data['Hold Times'] = data['Hold Times'].apply(lambda x: scaler.fit_transform(x.reshape(-1, 1)).flatten())\n",
        "    data['Flight Times'] = data['Flight Times'].apply(lambda x: scaler.fit_transform(x.reshape(-1, 1)).flatten())\n",
        "\n",
        "    return data\n",
        "\n",
        "# Preprocess features\n",
        "preprocessed_data = preprocess_features(features)\n",
        "\n",
        "# Split the preprocessed data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the target values to numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "label_encoder_classes = label_encoder.classes_  # Store the current label encoder classes\n",
        "\n",
        "# Add the new label and update the label encoder\n",
        "new_label = 'bnry@gmail.cpm'\n",
        "if new_label not in label_encoder_classes:\n",
        "    label_encoder_classes = np.append(label_encoder_classes, new_label)  # Add the new label to the classes\n",
        "    label_encoder.classes_ = label_encoder_classes\n",
        "    y_train_encoded = label_encoder.transform(y_train)\n",
        "\n",
        "# Convert the target values of the test set to numerical labels using the updated label encoder\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Print unique labels after updating the label encoder\n",
        "print(\"Unique labels after adding the new label:\", np.unique(label_encoder.inverse_transform(y_train_encoded)))\n",
        "\n",
        "# Train the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(X_train['Hold Times'].shape[1],)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train['Hold Times'], y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test['Hold Times'], y_test_encoded))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test['Hold Times'], y_test_encoded)\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n",
        "\n",
        "\n",
        "# Make predictions on new data\n",
        "new_data1 = pd.read_csv('/custom_data.csv')\n",
        "new_data = preprocess_features(new_data1)  # Preprocess the new data\n",
        "predictions = model.predict(new_data['Hold Times'])\n",
        "\n",
        "# Convert the predicted values back to categorical labels\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "bOBcaUn_lkJI",
        "outputId": "32286d69-eb81-42f3-f73d-5990c71ac29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'sethilnathan11@gmail.com'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-dfb7d5f7df31>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Convert the target values of the test set to numerical labels using the updated label encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0my_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Print unique labels after updating the label encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y contains previously unseen labels: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_unknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 'sethilnathan11@gmail.com'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Masking\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the dataset\n",
        "dataframe = pd.read_csv('/custom_data.csv')\n",
        "\n",
        "dataframe['Hold Times'] = dataframe['Hold Times'].apply(lambda x : np.array(x))\n",
        "dataframe['Flight Times'] = dataframe['Flight Times'].apply(lambda x : np.array(x))\n",
        "dataframe['Press/Release Timings'] = dataframe['Press/Release Timings'].apply(lambda x : np.array(x))\n",
        "dataframe['Key Combinations'] = dataframe['Key Combinations'].apply(lambda x : np.array(x))\n",
        "\n",
        "# Stack all arrays together vertically\n",
        "stacked_data = np.vstack([dataframe[col].values for col in dataframe.columns.tolist()])\n",
        "\n",
        "# Convert the numpy array to a tensor\n",
        "tensor_data = tf.convert_to_tensor(stacked_data, dtype=tf.float32)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(tensor_data, labels, test_size=0.2, random_state=42)\n",
        "def list_to_padded_array(x):\n",
        "    max_len = max(map(len, x))  # Get the length of the longest list\n",
        "    padded = pad_sequences(x, maxlen=max_len, dtype='float32')  # Pad the lists\n",
        "    return padded\n",
        "\n",
        "def flatten_list(x):\n",
        "    # If x is a tuple, convert it to a list\n",
        "    if isinstance(x, tuple):\n",
        "        x = list(x)\n",
        "\n",
        "    # If x is a list, flatten it\n",
        "    if isinstance(x, list):\n",
        "        return [item for sublist in x for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "    else:\n",
        "        return [x]\n",
        "\n",
        "def preprocess_features(data):\n",
        "    data = data.copy()\n",
        "\n",
        "    for column in ['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations']:\n",
        "        # Convert string of lists to actual lists using literal_eval\n",
        "        data[column] = data[column].apply(literal_eval)\n",
        "\n",
        "        # Check if the column is iterable\n",
        "        if isinstance(data[column][0], int):\n",
        "            continue\n",
        "\n",
        "        # Flatten the list of lists using list comprehension\n",
        "        data[column] = data[column].apply(flatten_list)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Preprocess features\n",
        "preprocessed_data = preprocess_features(data[['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations']])\n",
        "\n",
        "# Convert the target values to numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "target = label_encoder.fit_transform(data['Email'])\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(tensor_data, labels, test_size=0.2, random_state=42)\n",
        "# Add a Masking layer and then the rest of your model\n",
        "model = Sequential()\n",
        "model.add(Masking(mask_value=0., input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(len(set(target)), activation='softmax'))\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "9Wd2D1aJrKAT",
        "outputId": "f860fb93-eadf-4913-c69e-dd2f57f6909c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-03de702f22b7>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Convert the numpy array to a tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtensor_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Split the data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Windsor@123'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Masking\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/custom_data.csv')\n",
        "\n",
        "# Define function definitions\n",
        "\n",
        "def list_to_padded_array(x):\n",
        "    # Ensure x is a list of lists\n",
        "    if isinstance(x, int):\n",
        "        x = [[x]]\n",
        "    else:\n",
        "        x = [[val] if isinstance(val, int) else val for val in x]\n",
        "\n",
        "    # Check if the column contains non-numeric values\n",
        "    if any(not isinstance(val, (int, float)) for sublist in x for val in sublist):\n",
        "        # Convert non-numeric values to one-hot encoded values\n",
        "        label_encoder = LabelEncoder()\n",
        "        x = [label_encoder.fit_transform(sublist) for sublist in x]\n",
        "\n",
        "    max_len = max(map(len, x))  # Get the length of the longest list\n",
        "    padded = pad_sequences(x, maxlen=max_len, dtype='float32')  # Pad the lists\n",
        "\n",
        "    return padded\n",
        "def flatten_list(l):\n",
        "    flat_list = []\n",
        "    for sublist in l:\n",
        "        if isinstance(sublist, list):\n",
        "            for item in sublist:\n",
        "                flat_list.append(item)\n",
        "        else:\n",
        "            flat_list.append(sublist)\n",
        "    return flat_list\n",
        "\n",
        "def preprocess_features(data):\n",
        "    data = data.copy()\n",
        "\n",
        "    for column in ['Hold Times', 'Flight Times', 'Press/Release Timings']:\n",
        "        # Convert string of lists to actual lists using literal_eval\n",
        "        data[column] = data[column].apply(literal_eval)\n",
        "\n",
        "        # Check if the column is iterable\n",
        "        if isinstance(data[column][0], int):\n",
        "            continue\n",
        "\n",
        "        # Flatten the list of lists using list comprehension\n",
        "        data[column] = data[column].apply(lambda x: np.array(flatten_list(x)).astype(float))\n",
        "\n",
        "    # Convert 'Key Combinations' column to string representation\n",
        "    data['Key Combinations'] = data['Key Combinations'].astype(str)\n",
        "\n",
        "    # Apply LabelEncoder to 'Key Combinations' column\n",
        "    label_encoder = LabelEncoder()\n",
        "    data['Key Combinations'] = label_encoder.fit_transform(data['Key Combinations'])\n",
        "\n",
        "    return data\n",
        "\n",
        "# Preprocess features\n",
        "preprocessed_data = preprocess_features(data[['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations']])\n",
        "\n",
        "# Convert the target values to numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "target = label_encoder.fit_transform(data['Email'])\n",
        "\n",
        "# Split the preprocessed data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert sequences within array elements to tensors individually\n",
        "X_train_list = X_train.apply(lambda x: [list(i) if isinstance(i, np.ndarray) else i for i in x])\n",
        "X_test_list = X_test.apply(lambda x: [list(i) if isinstance(i, np.ndarray) else i for i in x])\n",
        "\n",
        "# The dtype of sequences should be 'float32' as you mentioned in pad_sequences.\n",
        "# Ensure elements of x are float, if not convert them.\n",
        "X_train_list = X_train_list.apply(lambda x: [float(i) for i in x])\n",
        "X_test_list = X_test_list.apply(lambda x: [float(i) for i in x])\n",
        "\n",
        "# Now apply pad_sequences\n",
        "X_train_pad = pad_sequences(X_train_list.values, dtype='float32')\n",
        "X_test_pad = pad_sequences(X_test_list.values, dtype='float32')\n",
        "\n",
        "y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
        "y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Masking(mask_value=0., input_shape=X_train[0].shape))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(len(set(target)), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bYrMVvk2B6Eo",
        "outputId": "34862776-102e-4402-f54c-8efef3e5ea4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5325dc1fac43>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# The dtype of sequences should be 'float32' as you mentioned in pad_sequences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Ensure elements of x are float, if not convert them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mX_train_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mX_test_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9566\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9567\u001b[0m         )\n\u001b[0;32m-> 9568\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9570\u001b[0m     def applymap(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5325dc1fac43>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# The dtype of sequences should be 'float32' as you mentioned in pad_sequences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Ensure elements of x are float, if not convert them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mX_train_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mX_test_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5325dc1fac43>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# The dtype of sequences should be 'float32' as you mentioned in pad_sequences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Ensure elements of x are float, if not convert them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mX_train_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mX_test_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "data[['Hold Times', 'Flight Times', 'Key Combinations']] = scaler.fit_transform(data[['Hold Times', 'Flight Times', 'Key Combinations']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JtdcCPmhU879",
        "outputId": "6e4dd1cd-932f-4f3a-d108-fb1aed1713d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-37548b77b6f1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Flight Times'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Key Combinations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Flight Times'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Key Combinations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_pass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy.array_api\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2070\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m     def __array_wrap__(\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '[42, 1176, 135, 108, 95, 78, 110, 193, 67, 137, 81, 785, 500, 93, 0, 314, 78, 149, 108, 87, 54, 117, 76, 120, 69, 195, 86, 154, 87, 123, 77, 138, 76, 174, 98, 454, 345, 108, 2, 140, 79, 102, 97, 47, 99, 82, 118, 15, 77, 107, 99, 710, 192, 96, 17, 187, 91, 125, 91, 131, 110]'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "FFHEDqaBnEqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "data[['Hold Times', 'Flight Times', 'Key Combinations']] = scaler.fit_transform(data[['Hold Times', 'Flight Times', 'Key Combinations']])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf_rf = RandomForestClassifier()\n",
        "clf_rf.fit(X_train, y_train)\n",
        "\n",
        "clf_dt = DecisionTreeClassifier()\n",
        "clf_dt.fit(X_train, y_train)\n",
        "\n",
        "clf_svm = svm.SVC()\n",
        "clf_svm.fit(X_train, y_train)\n",
        "\n",
        "clf_nn = MLPClassifier()\n",
        "clf_nn.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred_rf = clf_rf.predict(X_test)\n",
        "print('Random Forest accuracy: ', accuracy_score(y_test, y_pred_rf))\n",
        "\n",
        "y_pred_dt = clf_dt.predict(X_test)\n",
        "print('Decision Tree accuracy: ', accuracy_score(y_test, y_pred_dt))\n",
        "\n",
        "y_pred_svm = clf_svm.predict(X_test)\n",
        "print('SVM accuracy: ', accuracy_score(y_test, y_pred_svm))\n",
        "\n",
        "y_pred_nn = clf_nn.predict(X_test)\n",
        "print('Neural Network accuracy: ', accuracy_score(y_test, y_pred_nn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0np_WmH1njE0",
        "outputId": "2ba5c7e0-4891-4533-fd9c-2703a2348b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4f4256c46467>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Flight Times'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Key Combinations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hold Times'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Flight Times'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Key Combinations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_pass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy.array_api\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2070\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m     def __array_wrap__(\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '[42, 1176, 135, 108, 95, 78, 110, 193, 67, 137, 81, 785, 500, 93, 0, 314, 78, 149, 108, 87, 54, 117, 76, 120, 69, 195, 86, 154, 87, 123, 77, 138, 76, 174, 98, 454, 345, 108, 2, 140, 79, 102, 97, 47, 99, 82, 118, 15, 77, 107, 99, 710, 192, 96, 17, 187, 91, 125, 91, 131, 110]'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Masking\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/custom_data.csv')\n",
        "\n",
        "# Define function to convert string of lists to actual lists\n",
        "def convert_to_list(x):\n",
        "    return literal_eval(x)\n",
        "\n",
        "# Convert string columns to actual lists\n",
        "string_columns = ['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations']\n",
        "for column in string_columns:\n",
        "    data[column] = data[column].apply(convert_to_list)\n",
        "\n",
        "# Convert 'Key Combinations' column to string representation\n",
        "data['Key Combinations'] = data['Key Combinations'].astype(str)\n",
        "\n",
        "# Apply LabelEncoder to 'Key Combinations' column\n",
        "label_encoder = LabelEncoder()\n",
        "data['Key Combinations'] = label_encoder.fit_transform(data['Key Combinations'])\n",
        "\n",
        "# Split features and target\n",
        "X = data[string_columns]\n",
        "y = label_encoder.fit_transform(data['Email'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define function to convert sequences to tensors\n",
        "def convert_to_tensor(x):\n",
        "    return tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "\n",
        "# Convert sequences to tensors\n",
        "X_train = X_train.applymap(convert_to_tensor)\n",
        "X_test = X_test.applymap(convert_to_tensor)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Masking(mask_value=0., input_shape=X_train['Hold Times'].values[0].shape))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(len(set(y)), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "PeUB5L5fnrlb",
        "outputId": "de52d9bd-196d-4df9-e069-7b5546be4378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-11e24aeebfc1>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type tensorflow.python.framework.ops.EagerTensor)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/custom_data.csv')\n",
        "\n",
        "# Extract features from 'Hold Times' column\n",
        "data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "\n",
        "# Extract features from 'Hold Times' column\n",
        "data['Hold Times_Max'] = data['Hold Times'].apply(lambda x: np.max([float(val) for val in x]))\n",
        "data['Hold Times_Min'] = data['Hold Times'].apply(lambda x: np.min([float(val) for val in x]))\n",
        "data['Hold Times_Mean'] = data['Hold Times'].apply(lambda x: np.mean([float(val) for val in x]))\n",
        "data['Hold Times_Std'] = data['Hold Times'].apply(lambda x: np.std([float(val) for val in x]))\n",
        "data['Hold Times_Sum'] = data['Hold Times'].apply(lambda x: np.sum([float(val) for val in x]))\n",
        "data['Hold Times_Length'] = data['Hold Times'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# Extract features from 'Flight Times' column\n",
        "data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "\n",
        "# Extract features from 'Flight Times' column\n",
        "data['Flight Times_Max'] = data['Flight Times'].apply(lambda x: np.max([float(val) for val in x]))\n",
        "data['Flight Times_Min'] = data['Flight Times'].apply(lambda x: np.min([float(val) for val in x]))\n",
        "data['Flight Times_Mean'] = data['Flight Times'].apply(lambda x: np.mean([float(val) for val in x]))\n",
        "data['Flight Times_Std'] = data['Flight Times'].apply(lambda x: np.std([float(val) for val in x]))\n",
        "data['Flight Times_Sum'] = data['Flight Times'].apply(lambda x: np.sum([float(val) for val in x]))\n",
        "data['Flight Times_Length'] = data['Flight Times'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# Extract features from 'Press/Release Timings' column\n",
        "data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "\n",
        "# Extract features from 'Press/Release Timings' column\n",
        "data['Press/Release Timings_Max'] = data['Press/Release Timings'].apply(lambda x: np.max([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Min'] = data['Press/Release Timings'].apply(lambda x: np.min([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Mean'] = data['Press/Release Timings'].apply(lambda x: np.mean([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Std'] = data['Press/Release Timings'].apply(lambda x: np.std([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Sum'] = data['Press/Release Timings'].apply(lambda x: np.sum([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Length'] = data['Press/Release Timings'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# Extract features from 'Key Combinations' column\n",
        "data['Key Combinations_Unique_Count'] = data['Key Combinations'].apply(lambda x: len(set(x)))\n",
        "data['Key Combinations_Length'] = data['Key Combinations'].apply(lambda x: len(x))\n",
        "\n",
        "# Display the updated dataset with extracted features\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_e_X9VTuMiJ",
        "outputId": "9a68d27a-3ee2-4e9f-9a48-ee6ab532e272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      ID     Password              Email  \\\n",
            "0     52  Windsor@123    abdul@gmail.com   \n",
            "1     52  Windsor@123    abdul@gmail.com   \n",
            "2     52  Windsor@123    abdul@gmail.com   \n",
            "3     52  Windsor@123    abdul@gmail.com   \n",
            "4     52  Windsor@123    abdul@gmail.com   \n",
            "...   ..          ...                ...   \n",
            "1042  11  Windsor@123  zewbare@gmail.com   \n",
            "1043  11  Windsor@123  zewbare@gmail.com   \n",
            "1044  11  Windsor@123  zewbare@gmail.com   \n",
            "1045  11  Windsor@123  zewbare@gmail.com   \n",
            "1046  11  Windsor@123  zewbare@gmail.com   \n",
            "\n",
            "                                             Hold Times  \\\n",
            "0     [42, 1176, 135, 108, 95, 78, 110, 193, 67, 137...   \n",
            "1     [105, 245, 103, 116, 98, 159, 70, 133, 75, 204...   \n",
            "2     [135, 118, 83, 130, 116, 101, 70, 122, 89, 623...   \n",
            "3     [124, 96, 70, 97, 114, 107, 55, 153, 76, 587, ...   \n",
            "4     [125, 68, 73, 66, 100, 175, 93, 142, 75, 117, ...   \n",
            "...                                                 ...   \n",
            "1042  [162, 126, 116, 88, 59, 12, 108, 1209, 132, 96...   \n",
            "1043  [107, 145, 65, 99, 39, 8, 93, 113, 151, 7, 96,...   \n",
            "1044  [152, 64, 65, 118, 44, 28, 119, 20, 169, 24, 1...   \n",
            "1045  [107, 144, 125, 82, 72, 44, 81, 20, 156, 72, 4...   \n",
            "1046  [154, 99, 88, 116, 79, 456, 40, 68, 177, 104, ...   \n",
            "\n",
            "                                           Flight Times  \\\n",
            "0     [135, 95, 110, 67, 81, 93, 78, 108, 54, 76, 69...   \n",
            "1     [105, 103, 98, 70, 75, 117, 82, 87, 100, 59, 7...   \n",
            "2     [135, 83, 116, 70, 89, 97, 71, 76, 99, 55, 92,...   \n",
            "3     [124, 70, 114, 55, 76, 114, 71, 71, 118, 51, 6...   \n",
            "4     [125, 73, 100, 93, 75, 73, 86, 113, 56, 73, 92...   \n",
            "...                                                 ...   \n",
            "1042  [162, 88, 12, 132, 60, 100, 87, 92, 135, 76, 8...   \n",
            "1043  [107, 99, 93, 151, 103, 99, 104, 100, 20, 72, ...   \n",
            "1044  [152, 118, 20, 91, 118, 38, 108, 96, 136, 135,...   \n",
            "1045  [107, 82, 20, 116, 75, 112, 96, 36, 91, 89, 27...   \n",
            "1046  [154, 116, 40, 177, 127, 68, 108, 95, 16, 55, ...   \n",
            "\n",
            "                                  Press/Release Timings  \\\n",
            "0     [(1218, 1353), (1461, 1556), (1634, 1744), (19...   \n",
            "1     [(0, 105), (350, 453), (569, 667), (826, 896),...   \n",
            "2     [(0, 135), (253, 336), (466, 582), (683, 753),...   \n",
            "3     [(0, 124), (220, 290), (387, 501), (608, 663),...   \n",
            "4     [(0, 125), (193, 266), (332, 432), (607, 700),...   \n",
            "...                                                 ...   \n",
            "1042  [(0, 162), (404, 492), (551, 563), (1880, 2012...   \n",
            "1043  [(0, 107), (317, 416), (463, 556), (669, 820),...   \n",
            "1044  [(0, 152), (281, 399), (590, 610), (924, 1015)...   \n",
            "1045  [(0, 107), (376, 458), (655, 675), (947, 1063)...   \n",
            "1046  [(0, 154), (341, 457), (992, 1032), (1100, 127...   \n",
            "\n",
            "                                       Key Combinations  Total Hold Time  \\\n",
            "0     [('a', 'a'), ('b', 'b'), ('d', 'd'), ('u', 'u'...             9701   \n",
            "1     [('a', 'a'), ('b', 'b'), ('d', 'd'), ('u', 'u'...             9245   \n",
            "2     [('a', 'a'), ('b', 'b'), ('d', 'd'), ('u', 'u'...             9483   \n",
            "3     [('a', 'a'), ('b', 'b'), ('d', 'd'), ('u', 'u'...             9452   \n",
            "4     [('a', 'a'), ('b', 'b'), ('d', 'd'), ('u', 'u'...             9864   \n",
            "...                                                 ...              ...   \n",
            "1042  [('z', 'z'), ('w', 'e'), ('b', 'w'), ('a', 'a'...             7364   \n",
            "1043  [('z', 'z'), ('w', 'e'), ('b', 'b'), ('a', 'a'...             8482   \n",
            "1044  [('z', 'z'), ('w', 'e'), ('a', 'b'), ('e', 'r'...             8327   \n",
            "1045  [('z', 'z'), ('w', 'e'), ('a', 'b'), ('e', 'r'...             7491   \n",
            "1046  [('z', 'z'), ('w', 'e'), ('b', 'b'), ('a', 'a'...             6781   \n",
            "\n",
            "      Total Flight Time  Total Key Combinations  ...  Flight Times_Sum  \\\n",
            "0                  2455                      27  ...            2455.0   \n",
            "1                  2540                      28  ...            2540.0   \n",
            "2                  2646                      29  ...            2646.0   \n",
            "3                  2510                      29  ...            2510.0   \n",
            "4                  2681                      31  ...            2681.0   \n",
            "...                 ...                     ...  ...               ...   \n",
            "1042               2097                      24  ...            2097.0   \n",
            "1043               1839                      21  ...            1839.0   \n",
            "1044               2497                      30  ...            2497.0   \n",
            "1045               1734                      22  ...            1734.0   \n",
            "1046               1945                      23  ...            1945.0   \n",
            "\n",
            "      Flight Times_Length  Press/Release Timings_Max  \\\n",
            "0                      27                     9591.0   \n",
            "1                      28                     9149.0   \n",
            "2                      29                     9388.0   \n",
            "3                      29                     9357.0   \n",
            "4                      31                     9794.0   \n",
            "...                   ...                        ...   \n",
            "1042                   24                     7263.0   \n",
            "1043                   21                     8379.0   \n",
            "1044                   30                     8219.0   \n",
            "1045                   22                     7399.0   \n",
            "1046                   23                     6681.0   \n",
            "\n",
            "      Press/Release Timings_Min  Press/Release Timings_Mean  \\\n",
            "0                        1218.0                 5518.407407   \n",
            "1                           0.0                 4756.928571   \n",
            "2                           0.0                 4353.344828   \n",
            "3                           0.0                 4754.793103   \n",
            "4                           0.0                 4964.903226   \n",
            "...                         ...                         ...   \n",
            "1042                        0.0                 3958.375000   \n",
            "1043                        0.0                 3654.952381   \n",
            "1044                        0.0                 4261.033333   \n",
            "1045                        0.0                 3996.863636   \n",
            "1046                        0.0                 3555.173913   \n",
            "\n",
            "      Press/Release Timings_Std  Press/Release Timings_Sum  \\\n",
            "0                   2482.645457                   148997.0   \n",
            "1                   2796.269261                   133194.0   \n",
            "2                   2650.000550                   126247.0   \n",
            "3                   2753.188326                   137889.0   \n",
            "4                   2782.755786                   153912.0   \n",
            "...                         ...                        ...   \n",
            "1042                2005.851872                    95001.0   \n",
            "1043                2483.343723                    76754.0   \n",
            "1044                2349.353847                   127831.0   \n",
            "1045                2282.470995                    87931.0   \n",
            "1046                1948.314537                    81769.0   \n",
            "\n",
            "      Press/Release Timings_Length  Key Combinations_Unique_Count  \\\n",
            "0                               27                             31   \n",
            "1                               28                             33   \n",
            "2                               29                             31   \n",
            "3                               29                             31   \n",
            "4                               31                             36   \n",
            "...                            ...                            ...   \n",
            "1042                            24                             29   \n",
            "1043                            21                             29   \n",
            "1044                            30                             32   \n",
            "1045                            22                             29   \n",
            "1046                            23                             29   \n",
            "\n",
            "      Key Combinations_Length  \n",
            "0                         336  \n",
            "1                         360  \n",
            "2                         368  \n",
            "3                         360  \n",
            "4                         408  \n",
            "...                       ...  \n",
            "1042                      292  \n",
            "1043                      256  \n",
            "1044                      426  \n",
            "1045                      268  \n",
            "1046                      280  \n",
            "\n",
            "[1047 rows x 30 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/custom_data.csv')\n",
        "\n",
        "# Preprocessing steps\n",
        "def preprocess_data(data):\n",
        "    # Drop unnecessary columns\n",
        "    data = data.drop(['ID', 'Password', 'Email'], axis=1)\n",
        "\n",
        "    # Preprocess Hold Times column\n",
        "    hold_times_max = data['Hold Times'].apply(lambda x: np.max(eval(x)))\n",
        "    hold_times_min = data['Hold Times'].apply(lambda x: np.min(eval(x)))\n",
        "    hold_times_mean = data['Hold Times'].apply(lambda x: np.mean(eval(x)))\n",
        "\n",
        "    # Preprocess Flight Times column\n",
        "    flight_times_max = data['Flight Times'].apply(lambda x: np.max(eval(x)))\n",
        "    flight_times_min = data['Flight Times'].apply(lambda x: np.min(eval(x)))\n",
        "    flight_times_mean = data['Flight Times'].apply(lambda x: np.mean(eval(x)))\n",
        "\n",
        "    # Preprocess Press/Release Timings column\n",
        "    press_release_max = data['Press/Release Timings'].apply(lambda x: np.max([val[0] for val in eval(x)]))\n",
        "    press_release_min = data['Press/Release Timings'].apply(lambda x: np.min([val[0] for val in eval(x)]))\n",
        "    press_release_mean = data['Press/Release Timings'].apply(lambda x: np.mean([val[0] for val in eval(x)]))\n",
        "\n",
        "    # Preprocess Key Combinations column\n",
        "    key_combinations_length = data['Key Combinations'].apply(lambda x: len(eval(x)))\n",
        "\n",
        "    # Create new preprocessed columns\n",
        "    data['Hold Times_Max'] = hold_times_max\n",
        "    data['Hold Times_Min'] = hold_times_min\n",
        "    data['Hold Times_Mean'] = hold_times_mean\n",
        "    data['Flight Times_Max'] = flight_times_max\n",
        "    data['Flight Times_Min'] = flight_times_min\n",
        "    data['Flight Times_Mean'] = flight_times_mean\n",
        "    data['Press/Release Timings_Max'] = press_release_max\n",
        "    data['Press/Release Timings_Min'] = press_release_min\n",
        "    data['Press/Release Timings_Mean'] = press_release_mean\n",
        "    data['Key Combinations_Length'] = key_combinations_length\n",
        "\n",
        "    # Drop original columns\n",
        "    data = data.drop(['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations'], axis=1)\n",
        "\n",
        "    # Scale numerical columns\n",
        "    numerical_columns = ['Total Hold Time', 'Total Flight Time', 'Total Key Combinations', 'Hold Times_Max',\n",
        "                         'Hold Times_Min', 'Hold Times_Mean', 'Flight Times_Max', 'Flight Times_Min',\n",
        "                         'Flight Times_Mean', 'Press/Release Timings_Max', 'Press/Release Timings_Min',\n",
        "                         'Press/Release Timings_Mean', 'Key Combinations_Length']\n",
        "    scaler = StandardScaler()\n",
        "    data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply preprocessing to the data\n",
        "preprocessed_data = preprocess_data(data)\n"
      ],
      "metadata": {
        "id": "1JAyzyrn2yUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Convert the preprocessed data to numpy arrays\n",
        "X = preprocessed_data.values\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "y = X[:, 0]  # Assuming the target column is in the first position\n",
        "X = X[:, 1:]  # Remove the target column from features\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the input data\n",
        "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
        "X_test = (X_test - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
        "\n",
        "# Define MLP model\n",
        "mlp_model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile and train MLP model\n",
        "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "mlp_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Reshape the input data for Siamese CNN\n",
        "X_train_reshape = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_reshape = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Define Siamese CNN model\n",
        "input_shape = (X_train_reshape.shape[1], 1)\n",
        "siamese_model = models.Sequential([\n",
        "    layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling1D(pool_size=2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile and train Siamese CNN model\n",
        "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "siamese_model.fit(X_train_reshape, y_train, epochs=10, batch_size=32, validation_data=(X_test_reshape, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YzY_uYg3mMU",
        "outputId": "d7cfb541-2e11-4555-e441-5c16c3bf1a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "27/27 [==============================] - 2s 18ms/step - loss: -0.1692 - accuracy: 0.0000e+00 - val_loss: -1.0182 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "27/27 [==============================] - 0s 7ms/step - loss: -1.8585 - accuracy: 0.0000e+00 - val_loss: -2.9410 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "27/27 [==============================] - 0s 4ms/step - loss: -4.3150 - accuracy: 0.0000e+00 - val_loss: -6.3737 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: -8.7571 - accuracy: 0.0000e+00 - val_loss: -12.5990 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: -16.9332 - accuracy: 0.0000e+00 - val_loss: -22.5929 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "27/27 [==============================] - 0s 2ms/step - loss: -29.7607 - accuracy: 0.0000e+00 - val_loss: -38.7131 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: -49.2420 - accuracy: 0.0000e+00 - val_loss: -61.4662 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "27/27 [==============================] - 0s 2ms/step - loss: -76.1621 - accuracy: 0.0000e+00 - val_loss: -94.3465 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: -113.1150 - accuracy: 0.0000e+00 - val_loss: -136.3481 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "27/27 [==============================] - 0s 2ms/step - loss: -161.6350 - accuracy: 0.0000e+00 - val_loss: -188.4656 - val_accuracy: 0.0000e+00\n",
            "Epoch 1/10\n",
            "27/27 [==============================] - 1s 9ms/step - loss: -0.4123 - accuracy: 0.0000e+00 - val_loss: -1.8768 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: -3.3689 - accuracy: 0.0000e+00 - val_loss: -6.2156 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: -9.8859 - accuracy: 0.0000e+00 - val_loss: -15.6015 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: -23.4775 - accuracy: 0.0000e+00 - val_loss: -35.0511 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: -49.7103 - accuracy: 0.0000e+00 - val_loss: -68.7984 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: -93.8686 - accuracy: 0.0000e+00 - val_loss: -123.1518 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: -160.8004 - accuracy: 0.0000e+00 - val_loss: -204.6372 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "27/27 [==============================] - 0s 4ms/step - loss: -255.7012 - accuracy: 0.0000e+00 - val_loss: -319.2858 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: -386.6905 - accuracy: 0.0000e+00 - val_loss: -468.5391 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: -556.7419 - accuracy: 0.0000e+00 - val_loss: -662.1781 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7a0ebb661f00>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Input, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Convert the preprocessed data to numpy arrays\n",
        "X = preprocessed_data[['Total Hold Time', 'Total Flight Time', 'Total Key Combinations',\n",
        "       'Hold Times_Max', 'Hold Times_Min', 'Hold Times_Mean',\n",
        "       'Flight Times_Max', 'Flight Times_Min', 'Flight Times_Mean',\n",
        "       'Press/Release Timings_Max', 'Press/Release Timings_Min',\n",
        "       'Press/Release Timings_Mean', 'Key Combinations_Length']].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# MLP Model\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(256, 128), activation='relu', solver='adam', random_state=42)\n",
        "mlp.fit(X_train_scaled, y_train_encoded)\n",
        "mlp_accuracy = mlp.score(X_test_scaled, y_test_encoded)\n",
        "print(\"MLP Accuracy:\", mlp_accuracy)\n",
        "\n",
        "# Siamese CNN Model\n",
        "input_shape = (X_train.shape[1],)\n",
        "input_layer = Input(shape=input_shape)\n",
        "\n",
        "# Embedding branch\n",
        "embedding_branch = Dense(128, activation='relu')(input_layer)\n",
        "embedding_model = Model(inputs=input_layer, outputs=embedding_branch)\n",
        "\n",
        "# Siamese branch\n",
        "siamese_input_1 = Input(shape=input_shape)\n",
        "siamese_input_2 = Input(shape=input_shape)\n",
        "\n",
        "embedding_output_1 = embedding_model(siamese_input_1)\n",
        "embedding_output_2 = embedding_model(siamese_input_2)\n",
        "\n",
        "concatenated = concatenate([embedding_output_1, embedding_output_2])\n",
        "dense_1 = Dense(64, activation='relu')(concatenated)\n",
        "output_layer = Dense(1, activation='sigmoid')(dense_1)\n",
        "\n",
        "siamese_model = Model(inputs=[siamese_input_1, siamese_input_2], outputs=output_layer)\n",
        "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "siamese_model.fit([X_train_scaled, X_train_scaled], y_train, epochs=10, batch_size=32, validation_data=([X_test_scaled, X_test_scaled], y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "3BLgbtxp4gJO",
        "outputId": "3fa1d836-6713-4244-9b5f-07deae3a987b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-382ac713ab27>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0my_train_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0my_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# MLP Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_unknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y contains previously unseen labels: {str(diff)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: [-0.7399680326205633, -0.6982583080861564, -0.6948847274252853, -0.6912555724719238, -0.6741832097335759, -0.668918379308277, -0.6585931778916714, -0.654912908079812, -0.6538394960513529, -0.6533794623248705, -0.6515393274189408, -0.6300199719912627, -0.6290487896797998, -0.6159633859042996, -0.611005244629989, -0.610085177177024, -0.5995555163264262, -0.5925016658536957, -0.5841188290600159, -0.5731291344829356, -0.5690910606615899, -0.5645929531137617, -0.5614238318868827, -0.5598392712734432, -0.559685926697949, -0.556363460895576, -0.555290048867117, -0.5491051509888532, -0.5389844090062397, -0.5378087672607846, -0.5256945457967473, -0.5181295467390362, -0.5099000545208505, -0.5020283663121512, -0.5001882314062215, -0.49952373824574686, -0.49349218494297725, -0.4902719488576002, -0.48071347031846534, -0.46727026253347875, -0.46604350592952565, -0.4645100601745842, -0.46389668187260763, -0.44968675121015034, -0.44472860993583974, -0.4441152316338632, -0.4208579710172514, -0.4185578023848393, -0.4081814861097356, -0.4019454733729738, -0.39933861558957334, -0.3982140887026163, -0.39576057549471, -0.39478939318324713, -0.3928470285603213, -0.3923869948338389, -0.3832885500211864, -0.3822151379927274, -0.380732807096284, -0.37413899035003584, -0.326142138220369, -0.323995314163451, -0.3233308210029764, -0.3017092358583022, -0.2732382596748896, -0.2709892059009755, -0.2549391403325886, -0.23735562900926013, -0.23715..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/custom_data.csv')\n",
        "\n",
        "# Preprocessing steps\n",
        "def preprocess_data(data):\n",
        "    # Drop unnecessary columns\n",
        "    data = data.drop(['Password', 'Email'], axis=1)\n",
        "\n",
        "    # Preprocess Hold Times column\n",
        "    hold_times_max = data['Hold Times'].apply(lambda x: np.max(eval(x)))\n",
        "    hold_times_min = data['Hold Times'].apply(lambda x: np.min(eval(x)))\n",
        "    hold_times_mean = data['Hold Times'].apply(lambda x: np.mean(eval(x)))\n",
        "\n",
        "    # Preprocess Flight Times column\n",
        "    flight_times_max = data['Flight Times'].apply(lambda x: np.max(eval(x)))\n",
        "    flight_times_min = data['Flight Times'].apply(lambda x: np.min(eval(x)))\n",
        "    flight_times_mean = data['Flight Times'].apply(lambda x: np.mean(eval(x)))\n",
        "\n",
        "    # Preprocess Press/Release Timings column\n",
        "    press_release_max = data['Press/Release Timings'].apply(lambda x: np.max([val[0] for val in eval(x)]))\n",
        "    press_release_min = data['Press/Release Timings'].apply(lambda x: np.min([val[0] for val in eval(x)]))\n",
        "    press_release_mean = data['Press/Release Timings'].apply(lambda x: np.mean([val[0] for val in eval(x)]))\n",
        "\n",
        "    # Preprocess Key Combinations column\n",
        "    key_combinations_length = data['Key Combinations'].apply(lambda x: len(eval(x)))\n",
        "\n",
        "    # Create new preprocessed columns\n",
        "    data['Hold Times_Max'] = hold_times_max\n",
        "    data['Hold Times_Min'] = hold_times_min\n",
        "    data['Hold Times_Mean'] = hold_times_mean\n",
        "    data['Flight Times_Max'] = flight_times_max\n",
        "    data['Flight Times_Min'] = flight_times_min\n",
        "    data['Flight Times_Mean'] = flight_times_mean\n",
        "    data['Press/Release Timings_Max'] = press_release_max\n",
        "    data['Press/Release Timings_Min'] = press_release_min\n",
        "    data['Press/Release Timings_Mean'] = press_release_mean\n",
        "    data['Key Combinations_Length'] = key_combinations_length\n",
        "\n",
        "    # Drop original columns\n",
        "    data = data.drop(['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations'], axis=1)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply preprocessing to the data\n",
        "preprocessed_data = preprocess_data(data)\n",
        "\n",
        "# Define features and labels\n",
        "label_encoder = LabelEncoder()\n",
        "target = label_encoder.fit_transform(target)\n",
        "\n",
        "# Split the data into Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build and train Multi-Layer Perceptron(MLP) model\n",
        "# Build and train Multi-Layer Perceptron(MLP) model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
        "\n",
        "# Test the model\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Accuracy: %.2f' % (accuracy * 100))\n",
        "\n",
        "# Test the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int).flatten()  # Convert predicted probabilities to binary labels (0 or 1)\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Convert binary labels back to original labels\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test_original, y_pred_original)\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1-score (for binary classification)\n",
        "precision = precision_score(y_test_original, y_pred_binary, average=None)\n",
        "recall = recall_score(y_test_original, y_pred_binary, average=None)\n",
        "f1 = f1_score(y_test_original, y_pred_binary, average=None)\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1-score:', f1)\n",
        "\n",
        "\n",
        "# Display confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test_original, y_pred_original)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "6smgkC3v8XMP",
        "outputId": "c8e87b8b-bd6d-4f83-8052-af1339817ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-fc426acd2e30>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Define features and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Split the data into Training set and Test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'target' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Convert the preprocessed data to numpy arrays\n",
        "X = preprocessed_data.values\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "y = np.zeros(X.shape[0])  # Initialize the target as zeros for a typical binary classification\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the input data\n",
        "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
        "X_test = (X_test - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
        "\n",
        "# Define Siamese CNN model\n",
        "input_shape = (X_train.shape[1],)\n",
        "input_left = tf.keras.layers.Input(shape=input_shape)\n",
        "input_right = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "siamese_cnn = models.Sequential([\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(8, activation='relu'),\n",
        "    layers.Dense(4, activation='relu')\n",
        "])\n",
        "\n",
        "encoded_left = siamese_cnn(input_left)\n",
        "encoded_right = siamese_cnn(input_right)\n",
        "\n",
        "# Calculate the Euclidean distance between the two encoded inputs\n",
        "distance = tf.keras.layers.Subtract()([encoded_left, encoded_right])\n",
        "distance = tf.keras.backend.abs(distance)  # Corrected line for calculating the absolute value\n",
        "\n",
        "# Output layer\n",
        "output = layers.Dense(1, activation='sigmoid')(distance)\n",
        "\n",
        "# Siamese model\n",
        "siamese_model = tf.keras.models.Model(inputs=[input_left, input_right], outputs=output)\n",
        "\n",
        "# Compile and train Siamese CNN model\n",
        "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "siamese_model.fit([X_train, X_train], y_train, epochs=10, batch_size=32, validation_data=([X_test, X_test], y_test))\n",
        "\n",
        "y_pred = siamese_model.predict([X_test, X_test])\n",
        "\n",
        "# Since it's a binary classification problem, convert probabilities to binary predictions (0 or 1)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print the predicted labels and the true labels\n",
        "print(\"Predicted Labels:\", y_pred_binary.flatten())\n",
        "print(\"True Labels:\", y_test)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = siamese_model.evaluate([X_test, X_test], y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NybtbesABDgg",
        "outputId": "363f1f09-b7d9-4467-a350-739023fcb52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "27/27 [==============================] - 3s 9ms/step - loss: 0.6869 - accuracy: 1.0000 - val_loss: 0.6798 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6736 - accuracy: 1.0000 - val_loss: 0.6666 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6606 - accuracy: 1.0000 - val_loss: 0.6537 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6478 - accuracy: 1.0000 - val_loss: 0.6411 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6353 - accuracy: 1.0000 - val_loss: 0.6287 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6231 - accuracy: 1.0000 - val_loss: 0.6166 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6111 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5993 - accuracy: 1.0000 - val_loss: 0.5931 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5878 - accuracy: 1.0000 - val_loss: 0.5818 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5766 - accuracy: 1.0000 - val_loss: 0.5706 - val_accuracy: 1.0000\n",
            "7/7 [==============================] - 0s 1ms/step\n",
            "Predicted Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "True Labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.5706 - accuracy: 1.0000\n",
            "Test Loss: 0.5706496238708496\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/custom_data18July .csv')\n",
        "\n",
        "# Extract features from 'Hold Times' column\n",
        "data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "\n",
        "# Extract features from 'Hold Times' column\n",
        "data['Hold Times_Max'] = data['Hold Times'].apply(lambda x: np.max([float(val) for val in x]))\n",
        "data['Hold Times_Min'] = data['Hold Times'].apply(lambda x: np.min([float(val) for val in x]))\n",
        "data['Hold Times_Mean'] = data['Hold Times'].apply(lambda x: np.mean([float(val) for val in x]))\n",
        "data['Hold Times_Std'] = data['Hold Times'].apply(lambda x: np.std([float(val) for val in x]))\n",
        "data['Hold Times_Sum'] = data['Hold Times'].apply(lambda x: np.sum([float(val) for val in x]))\n",
        "data['Hold Times_Length'] = data['Hold Times'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# Extract features from 'Flight Times' column\n",
        "data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "\n",
        "# Extract features from 'Flight Times' column\n",
        "data['Flight Times_Max'] = data['Flight Times'].apply(lambda x: np.max([float(val) for val in x]))\n",
        "data['Flight Times_Min'] = data['Flight Times'].apply(lambda x: np.min([float(val) for val in x]))\n",
        "data['Flight Times_Mean'] = data['Flight Times'].apply(lambda x: np.mean([float(val) for val in x]))\n",
        "data['Flight Times_Std'] = data['Flight Times'].apply(lambda x: np.std([float(val) for val in x]))\n",
        "data['Flight Times_Sum'] = data['Flight Times'].apply(lambda x: np.sum([float(val) for val in x]))\n",
        "data['Flight Times_Length'] = data['Flight Times'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# Extract features from 'Press/Release Timings' column\n",
        "data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "\n",
        "# Extract features from 'Press/Release Timings' column\n",
        "data['Press/Release Timings_Max'] = data['Press/Release Timings'].apply(lambda x: np.max([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Min'] = data['Press/Release Timings'].apply(lambda x: np.min([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Mean'] = data['Press/Release Timings'].apply(lambda x: np.mean([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Std'] = data['Press/Release Timings'].apply(lambda x: np.std([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Sum'] = data['Press/Release Timings'].apply(lambda x: np.sum([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Length'] = data['Press/Release Timings'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# Extract features from 'Key Combinations' column\n",
        "data['Key Combinations_Unique_Count'] = data['Key Combinations'].apply(lambda x: len(set(x)))\n",
        "data['Key Combinations_Length'] = data['Key Combinations'].apply(lambda x: len(x))\n",
        "\n",
        "# Display the updated dataset with extracted features\n",
        "# print(data)\n",
        "\n",
        "\n",
        "#PreProcessing it\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/custom_data18July .csv')\n",
        "\n",
        "# Preprocessing steps\n",
        "def preprocess_data(data):\n",
        "    # Drop unnecessary columns\n",
        "    data = data.drop(['Password', 'Email'], axis=1)\n",
        "\n",
        "    # Preprocess Hold Times column\n",
        "    hold_times_max = data['Hold Times'].apply(lambda x: np.max(eval(x)))\n",
        "    hold_times_min = data['Hold Times'].apply(lambda x: np.min(eval(x)))\n",
        "    hold_times_mean = data['Hold Times'].apply(lambda x: np.mean(eval(x)))\n",
        "\n",
        "    # Preprocess Flight Times column\n",
        "    flight_times_max = data['Flight Times'].apply(lambda x: np.max(eval(x)))\n",
        "    flight_times_min = data['Flight Times'].apply(lambda x: np.min(eval(x)))\n",
        "    flight_times_mean = data['Flight Times'].apply(lambda x: np.mean(eval(x)))\n",
        "\n",
        "    # Preprocess Press/Release Timings column\n",
        "    press_release_max = data['Press/Release Timings'].apply(lambda x: np.max([val[0] for val in eval(x)]))\n",
        "    press_release_min = data['Press/Release Timings'].apply(lambda x: np.min([val[0] for val in eval(x)]))\n",
        "    press_release_mean = data['Press/Release Timings'].apply(lambda x: np.mean([val[0] for val in eval(x)]))\n",
        "\n",
        "    # Preprocess Key Combinations column\n",
        "    key_combinations_length = data['Key Combinations'].apply(lambda x: len(eval(x)))\n",
        "\n",
        "    # Create new preprocessed columns\n",
        "    data['Hold Times_Max'] = hold_times_max\n",
        "    data['Hold Times_Min'] = hold_times_min\n",
        "    data['Hold Times_Mean'] = hold_times_mean\n",
        "    data['Flight Times_Max'] = flight_times_max\n",
        "    data['Flight Times_Min'] = flight_times_min\n",
        "    data['Flight Times_Mean'] = flight_times_mean\n",
        "    data['Press/Release Timings_Max'] = press_release_max\n",
        "    data['Press/Release Timings_Min'] = press_release_min\n",
        "    data['Press/Release Timings_Mean'] = press_release_mean\n",
        "    data['Key Combinations_Length'] = key_combinations_length\n",
        "\n",
        "    # Drop original columns\n",
        "    data = data.drop(['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations'], axis=1)\n",
        "\n",
        "    # Scale numerical columns\n",
        "    numerical_columns = ['Total Hold Time', 'Total Flight Time', 'Total Key Combinations', 'Hold Times_Max',\n",
        "                         'Hold Times_Min', 'Hold Times_Mean', 'Flight Times_Max', 'Flight Times_Min',\n",
        "                         'Flight Times_Mean', 'Press/Release Timings_Max', 'Press/Release Timings_Min',\n",
        "                         'Press/Release Timings_Mean', 'Key Combinations_Length']\n",
        "    scaler = StandardScaler()\n",
        "    data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply preprocessing to the data\n",
        "preprocessed_data = preprocess_data(data)\n",
        "\n",
        "# Models and evaluating it\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models, backend as K\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "# Convert the preprocessed data to numpy arrays\n",
        "X = preprocessed_data.values\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "y = preprocessed_data['ID']  # Initialize the target as zeros for a typical binary classification\n",
        "\n",
        "y_encoded = pd.get_dummies(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Normalize the input data\n",
        "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
        "X_test = (X_test - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
        "\n",
        "# Define Siamese CNN model\n",
        "input_shape = (X_train.shape[1],)\n",
        "input_left = tf.keras.layers.Input(shape=input_shape)\n",
        "input_right = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "siamese_cnn = models.Sequential([\n",
        "    layers.Dense(32, activation='relu'),\n",
        "\n",
        "    layers.Dense(64, activation='relu'),\n",
        "\n",
        "    layers.Dense(128, activation='relu'),\n",
        "\n",
        "    layers.Dense(256, activation='relu'),\n",
        "\n",
        "    layers.Dense(512, activation='relu'),\n",
        "])\n",
        "\n",
        "encoded_left = siamese_cnn(input_left)\n",
        "encoded_right = siamese_cnn(input_right)\n",
        "\n",
        "# Calculate the Euclidean distance between the two encoded inputs\n",
        "distance = layers.Subtract()([encoded_left, encoded_right])\n",
        "distance = layers.Lambda(lambda x: K.abs(x))(distance) # Corrected line for calculating the absolute value\n",
        "\n",
        "# Output layer\n",
        "num_classes = len(y_encoded.columns)\n",
        "output = layers.Dense(num_classes, activation='softmax')(distance)\n",
        "\n",
        "# Siamese model\n",
        "siamese_model = models.Model(inputs=[input_left, input_right], outputs=output)\n",
        "# sgd = SGD(learning_rate=0.01)\n",
        "# Compile and train Siamese CNN model\n",
        "siamese_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "siamese_model.fit([X_train, X_train], y_train, epochs=50, batch_size=32, validation_data=([X_test, X_test], y_test))\n",
        "\n",
        "y_pred = siamese_model.predict([X_test, X_test])\n",
        "\n",
        "# Since it's a binary classification problem, convert probabilities to binary predictions (0 or 1)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Get the true labels\n",
        "y_true_labels = np.argmax(y_test.values, axis=1)\n",
        "\n",
        "# Print the predicted labels and the true labels\n",
        "print(\"Predicted Labels:\", y_pred_labels)\n",
        "print(\"True Labels:\", y_true_labels)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = siamese_model.evaluate([X_test, X_test], y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REKyDgZHI_fF",
        "outputId": "44b37bde-d596-4f0c-c0f2-3641575194c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "131/131 [==============================] - 3s 10ms/step - loss: 3.9358 - accuracy: 0.0814 - val_loss: 3.9185 - val_accuracy: 0.0974\n",
            "Epoch 2/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.9069 - accuracy: 0.0843 - val_loss: 3.8911 - val_accuracy: 0.0974\n",
            "Epoch 3/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.8827 - accuracy: 0.0843 - val_loss: 3.8676 - val_accuracy: 0.0974\n",
            "Epoch 4/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.8616 - accuracy: 0.0843 - val_loss: 3.8470 - val_accuracy: 0.0974\n",
            "Epoch 5/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.8436 - accuracy: 0.0843 - val_loss: 3.8297 - val_accuracy: 0.0974\n",
            "Epoch 6/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.8280 - accuracy: 0.0843 - val_loss: 3.8142 - val_accuracy: 0.0974\n",
            "Epoch 7/50\n",
            "131/131 [==============================] - 1s 10ms/step - loss: 3.8147 - accuracy: 0.0843 - val_loss: 3.8009 - val_accuracy: 0.0974\n",
            "Epoch 8/50\n",
            "131/131 [==============================] - 2s 15ms/step - loss: 3.8034 - accuracy: 0.0843 - val_loss: 3.7897 - val_accuracy: 0.0974\n",
            "Epoch 9/50\n",
            "131/131 [==============================] - 2s 15ms/step - loss: 3.7936 - accuracy: 0.0843 - val_loss: 3.7798 - val_accuracy: 0.0974\n",
            "Epoch 10/50\n",
            "131/131 [==============================] - 1s 10ms/step - loss: 3.7852 - accuracy: 0.0843 - val_loss: 3.7716 - val_accuracy: 0.0974\n",
            "Epoch 11/50\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 3.7781 - accuracy: 0.0843 - val_loss: 3.7644 - val_accuracy: 0.0974\n",
            "Epoch 12/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7722 - accuracy: 0.0843 - val_loss: 3.7582 - val_accuracy: 0.0974\n",
            "Epoch 13/50\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 3.7671 - accuracy: 0.0843 - val_loss: 3.7531 - val_accuracy: 0.0974\n",
            "Epoch 14/50\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 3.7628 - accuracy: 0.0843 - val_loss: 3.7486 - val_accuracy: 0.0974\n",
            "Epoch 15/50\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 3.7592 - accuracy: 0.0843 - val_loss: 3.7450 - val_accuracy: 0.0974\n",
            "Epoch 16/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7561 - accuracy: 0.0843 - val_loss: 3.7419 - val_accuracy: 0.0974\n",
            "Epoch 17/50\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 3.7536 - accuracy: 0.0843 - val_loss: 3.7392 - val_accuracy: 0.0974\n",
            "Epoch 18/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7516 - accuracy: 0.0843 - val_loss: 3.7370 - val_accuracy: 0.0974\n",
            "Epoch 19/50\n",
            "131/131 [==============================] - 2s 14ms/step - loss: 3.7498 - accuracy: 0.0843 - val_loss: 3.7352 - val_accuracy: 0.0974\n",
            "Epoch 20/50\n",
            "131/131 [==============================] - 2s 15ms/step - loss: 3.7483 - accuracy: 0.0843 - val_loss: 3.7336 - val_accuracy: 0.0974\n",
            "Epoch 21/50\n",
            "131/131 [==============================] - 2s 14ms/step - loss: 3.7470 - accuracy: 0.0843 - val_loss: 3.7321 - val_accuracy: 0.0974\n",
            "Epoch 22/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7459 - accuracy: 0.0843 - val_loss: 3.7311 - val_accuracy: 0.0974\n",
            "Epoch 23/50\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 3.7450 - accuracy: 0.0843 - val_loss: 3.7302 - val_accuracy: 0.0974\n",
            "Epoch 24/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7441 - accuracy: 0.0843 - val_loss: 3.7293 - val_accuracy: 0.0974\n",
            "Epoch 25/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7435 - accuracy: 0.0843 - val_loss: 3.7286 - val_accuracy: 0.0974\n",
            "Epoch 26/50\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 3.7428 - accuracy: 0.0843 - val_loss: 3.7279 - val_accuracy: 0.0974\n",
            "Epoch 27/50\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 3.7424 - accuracy: 0.0843 - val_loss: 3.7274 - val_accuracy: 0.0974\n",
            "Epoch 28/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7419 - accuracy: 0.0843 - val_loss: 3.7270 - val_accuracy: 0.0974\n",
            "Epoch 29/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7414 - accuracy: 0.0843 - val_loss: 3.7265 - val_accuracy: 0.0974\n",
            "Epoch 30/50\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 3.7410 - accuracy: 0.0843 - val_loss: 3.7263 - val_accuracy: 0.0974\n",
            "Epoch 31/50\n",
            "131/131 [==============================] - 2s 14ms/step - loss: 3.7407 - accuracy: 0.0843 - val_loss: 3.7259 - val_accuracy: 0.0974\n",
            "Epoch 32/50\n",
            "131/131 [==============================] - 2s 15ms/step - loss: 3.7404 - accuracy: 0.0843 - val_loss: 3.7257 - val_accuracy: 0.0974\n",
            "Epoch 33/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7401 - accuracy: 0.0843 - val_loss: 3.7256 - val_accuracy: 0.0974\n",
            "Epoch 34/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7399 - accuracy: 0.0843 - val_loss: 3.7254 - val_accuracy: 0.0974\n",
            "Epoch 35/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7397 - accuracy: 0.0843 - val_loss: 3.7251 - val_accuracy: 0.0974\n",
            "Epoch 36/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7394 - accuracy: 0.0843 - val_loss: 3.7250 - val_accuracy: 0.0974\n",
            "Epoch 37/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7393 - accuracy: 0.0843 - val_loss: 3.7249 - val_accuracy: 0.0974\n",
            "Epoch 38/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7392 - accuracy: 0.0843 - val_loss: 3.7248 - val_accuracy: 0.0974\n",
            "Epoch 39/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7389 - accuracy: 0.0843 - val_loss: 3.7247 - val_accuracy: 0.0974\n",
            "Epoch 40/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7389 - accuracy: 0.0843 - val_loss: 3.7246 - val_accuracy: 0.0974\n",
            "Epoch 41/50\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 3.7387 - accuracy: 0.0843 - val_loss: 3.7245 - val_accuracy: 0.0974\n",
            "Epoch 42/50\n",
            "131/131 [==============================] - 2s 15ms/step - loss: 3.7386 - accuracy: 0.0843 - val_loss: 3.7245 - val_accuracy: 0.0974\n",
            "Epoch 43/50\n",
            "131/131 [==============================] - 2s 16ms/step - loss: 3.7386 - accuracy: 0.0843 - val_loss: 3.7245 - val_accuracy: 0.0974\n",
            "Epoch 44/50\n",
            "131/131 [==============================] - 2s 16ms/step - loss: 3.7384 - accuracy: 0.0843 - val_loss: 3.7243 - val_accuracy: 0.0974\n",
            "Epoch 45/50\n",
            "131/131 [==============================] - 2s 15ms/step - loss: 3.7383 - accuracy: 0.0843 - val_loss: 3.7243 - val_accuracy: 0.0974\n",
            "Epoch 46/50\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 3.7382 - accuracy: 0.0843 - val_loss: 3.7244 - val_accuracy: 0.0974\n",
            "Epoch 47/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7382 - accuracy: 0.0843 - val_loss: 3.7242 - val_accuracy: 0.0974\n",
            "Epoch 48/50\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 3.7382 - accuracy: 0.0843 - val_loss: 3.7243 - val_accuracy: 0.0974\n",
            "Epoch 49/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7380 - accuracy: 0.0843 - val_loss: 3.7241 - val_accuracy: 0.0974\n",
            "Epoch 50/50\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 3.7381 - accuracy: 0.0843 - val_loss: 3.7242 - val_accuracy: 0.0974\n",
            "33/33 [==============================] - 0s 3ms/step\n",
            "Predicted Labels: [0 0 0 ... 0 0 0]\n",
            "True Labels: [40  0  7 ... 16  8  5]\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 3.7242 - accuracy: 0.0974\n",
            "Test Loss: 3.724221706390381\n",
            "Test Accuracy: 0.09742120653390884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W5JDtX83QSuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### SCNN ARCHITECTURE ######\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/copy_data19July.csv')\n",
        "\n",
        "# Extract features from 'Hold Times' column\n",
        "data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "\n",
        "# Extract features from 'Hold Times' column\n",
        "data['Hold Times_Max'] = data['Hold Times'].apply(lambda x: np.max([float(val) for val in x]))\n",
        "data['Hold Times_Min'] = data['Hold Times'].apply(lambda x: np.min([float(val) for val in x]))\n",
        "data['Hold Times_Mean'] = data['Hold Times'].apply(lambda x: np.mean([float(val) for val in x]))\n",
        "data['Hold Times_Std'] = data['Hold Times'].apply(lambda x: np.std([float(val) for val in x]))\n",
        "data['Hold Times_Sum'] = data['Hold Times'].apply(lambda x: np.sum([float(val) for val in x]))\n",
        "data['Hold Times_Length'] = data['Hold Times'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# Extract features from 'Flight Times' column\n",
        "data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "\n",
        "# Extract features from 'Flight Times' column\n",
        "data['Flight Times_Max'] = data['Flight Times'].apply(lambda x: np.max([float(val) for val in x]))\n",
        "data['Flight Times_Min'] = data['Flight Times'].apply(lambda x: np.min([float(val) for val in x]))\n",
        "data['Flight Times_Mean'] = data['Flight Times'].apply(lambda x: np.mean([float(val) for val in x]))\n",
        "data['Flight Times_Std'] = data['Flight Times'].apply(lambda x: np.std([float(val) for val in x]))\n",
        "data['Flight Times_Sum'] = data['Flight Times'].apply(lambda x: np.sum([float(val) for val in x]))\n",
        "data['Flight Times_Length'] = data['Flight Times'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# Extract features from 'Press/Release Timings' column\n",
        "data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "\n",
        "# Extract features from 'Press/Release Timings' column\n",
        "data['Press/Release Timings_Max'] = data['Press/Release Timings'].apply(lambda x: np.max([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Min'] = data['Press/Release Timings'].apply(lambda x: np.min([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Mean'] = data['Press/Release Timings'].apply(lambda x: np.mean([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Std'] = data['Press/Release Timings'].apply(lambda x: np.std([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Sum'] = data['Press/Release Timings'].apply(lambda x: np.sum([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Length'] = data['Press/Release Timings'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# Extract features from 'Key Combinations' column\n",
        "data['Key Combinations_Unique_Count'] = data['Key Combinations'].apply(lambda x: len(set(x)))\n",
        "data['Key Combinations_Length'] = data['Key Combinations'].apply(lambda x: len(x))\n",
        "\n",
        "# Display the updated dataset with extracted features\n",
        "# print(data)\n",
        "\n",
        "\n",
        "#PreProcessing it\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/copy_data19July.csv')\n",
        "\n",
        "# Preprocessing steps\n",
        "# Preprocessing steps\n",
        "def preprocess_data(data):\n",
        "    # Drop unnecessary columns\n",
        "    data = data.drop(['Password', 'Email'], axis=1)\n",
        "\n",
        "    # Preprocess Hold Times column\n",
        "    hold_times_max = data['Hold Times'].apply(lambda x: np.max(eval(x)))\n",
        "    hold_times_min = data['Hold Times'].apply(lambda x: np.min(eval(x)))\n",
        "    hold_times_mean = data['Hold Times'].apply(lambda x: np.mean(eval(x)))\n",
        "\n",
        "    # Preprocess Flight Times column\n",
        "    flight_times_max = data['Flight Times'].apply(lambda x: np.max(eval(x)))\n",
        "    flight_times_min = data['Flight Times'].apply(lambda x: np.min(eval(x)))\n",
        "    flight_times_mean = data['Flight Times'].apply(lambda x: np.mean(eval(x)))\n",
        "\n",
        "    # Preprocess Press/Release Timings column\n",
        "    press_release_max = data['Press/Release Timings'].apply(lambda x: np.max([val[0] for val in eval(x)]))\n",
        "    press_release_min = data['Press/Release Timings'].apply(lambda x: np.min([val[0] for val in eval(x)]))\n",
        "    press_release_mean = data['Press/Release Timings'].apply(lambda x: np.mean([val[0] for val in eval(x)]))\n",
        "\n",
        "    # Create new preprocessed columns\n",
        "    data['Hold Times_Max'] = hold_times_max\n",
        "    data['Hold Times_Min'] = hold_times_min\n",
        "    data['Hold Times_Mean'] = hold_times_mean\n",
        "    data['Flight Times_Max'] = flight_times_max\n",
        "    data['Flight Times_Min'] = flight_times_min\n",
        "    data['Flight Times_Mean'] = flight_times_mean\n",
        "    data['Press/Release Timings_Max'] = press_release_max\n",
        "    data['Press/Release Timings_Min'] = press_release_min\n",
        "    data['Press/Release Timings_Mean'] = press_release_mean\n",
        "\n",
        "    # Drop original columns\n",
        "    data = data.drop(['Hold Times', 'Flight Times', 'Press/Release Timings'], axis=1)\n",
        "\n",
        "    # Scale numerical columns\n",
        "    numerical_columns = ['Total Hold Time', 'Total Flight Time', 'Hold Times_Max',\n",
        "                         'Hold Times_Min', 'Hold Times_Mean', 'Flight Times_Max', 'Flight Times_Min',\n",
        "                         'Flight Times_Mean', 'Press/Release Timings_Max', 'Press/Release Timings_Min',\n",
        "                         'Press/Release Timings_Mean']\n",
        "    scaler = StandardScaler()\n",
        "    data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "# Apply preprocessing to the data\n",
        "preprocessed_data = preprocess_data(data)\n",
        "\n",
        "# Models and evaluating it\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models, backend as K\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "# Convert the preprocessed data to numpy arrays\n",
        "X = preprocessed_data.values\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "y = preprocessed_data['ID']  # Initialize the target as zeros for a typical binary classification\n",
        "y_encoded = pd.get_dummies(y)\n",
        "\n",
        "# Convert target labels to numpy arrays\n",
        "y = preprocessed_data['ID']  # Initialize the target as zeros for a typical binary classification\n",
        "y_encoded = pd.get_dummies(y)\n",
        "\n",
        "# Split the data into training and testing sets with correct target labels\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify columns with string values (non-numerical) in X_train and X_test\n",
        "# Identify columns with string values (non-numerical) in X_train and X_test\n",
        "string_columns_train = X_train[:, [isinstance(val, str) for val in X_train[0]]]\n",
        "string_columns_test = X_test[:, [isinstance(val, str) for val in X_test[0]]]\n",
        "\n",
        "# Identify columns with numerical values in X_train and X_test\n",
        "numerical_columns_train = X_train[:, [isinstance(val, (int, float)) for val in X_train[0]]]\n",
        "numerical_columns_test = X_test[:, [isinstance(val, (int, float)) for val in X_test[0]]]\n",
        "\n",
        "# Convert numerical columns to numpy array with appropriate data type\n",
        "numerical_columns_train = np.array(numerical_columns_train, dtype=np.float32)\n",
        "numerical_columns_test = np.array(numerical_columns_test, dtype=np.float32)\n",
        "\n",
        "# Normalize only numerical columns\n",
        "numerical_columns_train = (numerical_columns_train - np.mean(numerical_columns_train, axis=0)) / np.std(numerical_columns_train, axis=0)\n",
        "numerical_columns_test = (numerical_columns_test - np.mean(numerical_columns_train, axis=0)) / np.std(numerical_columns_train, axis=0)\n",
        "\n",
        "# Perform one-hot encoding on the string columns\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "string_columns_train_encoded = encoder.fit_transform(string_columns_train).toarray()\n",
        "string_columns_test_encoded = encoder.transform(string_columns_test).toarray()\n",
        "\n",
        "# Concatenate back numerical and one-hot encoded string columns for X_train and X_test\n",
        "X_train = np.concatenate((numerical_columns_train, string_columns_train_encoded), axis=1)\n",
        "X_test = np.concatenate((numerical_columns_test, string_columns_test_encoded), axis=1)\n",
        "\n",
        "# Define Siamese CNN model\n",
        "input_shape = (X_train.shape[1],)\n",
        "input_left = tf.keras.layers.Input(shape=input_shape)\n",
        "input_right = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "siamese_cnn = models.Sequential([\n",
        "    layers.Dense(32, activation='relu'),\n",
        "\n",
        "    layers.Dense(64, activation='relu'),\n",
        "\n",
        "    layers.Dense(128, activation='relu'),\n",
        "\n",
        "    layers.Dense(256, activation='relu'),\n",
        "\n",
        "    layers.Dense(512, activation='relu'),\n",
        "])\n",
        "\n",
        "encoded_left = siamese_cnn(input_left)\n",
        "encoded_right = siamese_cnn(input_right)\n",
        "\n",
        "# Calculate the Euclidean distance between the two encoded inputs\n",
        "distance = layers.Subtract()([encoded_left, encoded_right])\n",
        "distance = layers.Lambda(lambda x: K.abs(x))(distance) # Corrected line for calculating the absolute value\n",
        "\n",
        "# Output layer\n",
        "num_classes = len(y_encoded.columns)\n",
        "output = layers.Dense(num_classes, activation='softmax')(distance)\n",
        "\n",
        "# Siamese model\n",
        "siamese_model = models.Model(inputs=[input_left, input_right], outputs=output)\n",
        "# sgd = SGD(learning_rate=0.01)\n",
        "# Compile and train Siamese CNN model\n",
        "siamese_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "siamese_model.fit([X_train, X_train], y_train, epochs=50, batch_size=32, validation_data=([X_test, X_test], y_test))\n",
        "\n",
        "y_pred = siamese_model.predict([X_test, X_test])\n",
        "\n",
        "# Since it's a binary classification problem, convert probabilities to binary predictions (0 or 1)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Get the true labels\n",
        "y_true_labels = np.argmax(y_test.values, axis=1)\n",
        "\n",
        "# Print the predicted labels and the true labels\n",
        "print(\"Predicted Labels:\", y_pred_labels)\n",
        "print(\"True Labels:\", y_true_labels)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = siamese_model.evaluate([X_test, X_test], y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhLSqIbH389b",
        "outputId": "e4b1aafa-4e33-4bde-d881-a37ae2133335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "550/550 [==============================] - 8s 12ms/step - loss: 3.8937 - accuracy: 0.0869 - val_loss: 3.8507 - val_accuracy: 0.0828\n",
            "Epoch 2/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.8153 - accuracy: 0.0880 - val_loss: 3.7981 - val_accuracy: 0.0828\n",
            "Epoch 3/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7740 - accuracy: 0.0880 - val_loss: 3.7717 - val_accuracy: 0.0828\n",
            "Epoch 4/50\n",
            "550/550 [==============================] - 7s 12ms/step - loss: 3.7530 - accuracy: 0.0880 - val_loss: 3.7592 - val_accuracy: 0.0828\n",
            "Epoch 5/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7430 - accuracy: 0.0880 - val_loss: 3.7535 - val_accuracy: 0.0828\n",
            "Epoch 6/50\n",
            "550/550 [==============================] - 7s 12ms/step - loss: 3.7380 - accuracy: 0.0880 - val_loss: 3.7508 - val_accuracy: 0.0828\n",
            "Epoch 7/50\n",
            "550/550 [==============================] - 6s 10ms/step - loss: 3.7355 - accuracy: 0.0880 - val_loss: 3.7493 - val_accuracy: 0.0828\n",
            "Epoch 8/50\n",
            "550/550 [==============================] - 7s 13ms/step - loss: 3.7340 - accuracy: 0.0880 - val_loss: 3.7483 - val_accuracy: 0.0828\n",
            "Epoch 9/50\n",
            "550/550 [==============================] - 7s 12ms/step - loss: 3.7331 - accuracy: 0.0880 - val_loss: 3.7477 - val_accuracy: 0.0828\n",
            "Epoch 10/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7325 - accuracy: 0.0880 - val_loss: 3.7472 - val_accuracy: 0.0828\n",
            "Epoch 11/50\n",
            "550/550 [==============================] - 7s 12ms/step - loss: 3.7320 - accuracy: 0.0880 - val_loss: 3.7470 - val_accuracy: 0.0828\n",
            "Epoch 12/50\n",
            "550/550 [==============================] - 5s 10ms/step - loss: 3.7318 - accuracy: 0.0880 - val_loss: 3.7468 - val_accuracy: 0.0828\n",
            "Epoch 13/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7316 - accuracy: 0.0880 - val_loss: 3.7465 - val_accuracy: 0.0828\n",
            "Epoch 14/50\n",
            "550/550 [==============================] - 7s 13ms/step - loss: 3.7314 - accuracy: 0.0880 - val_loss: 3.7464 - val_accuracy: 0.0828\n",
            "Epoch 15/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7313 - accuracy: 0.0880 - val_loss: 3.7464 - val_accuracy: 0.0828\n",
            "Epoch 16/50\n",
            "550/550 [==============================] - 6s 11ms/step - loss: 3.7312 - accuracy: 0.0880 - val_loss: 3.7463 - val_accuracy: 0.0828\n",
            "Epoch 17/50\n",
            "550/550 [==============================] - 6s 10ms/step - loss: 3.7312 - accuracy: 0.0880 - val_loss: 3.7463 - val_accuracy: 0.0828\n",
            "Epoch 18/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7312 - accuracy: 0.0880 - val_loss: 3.7462 - val_accuracy: 0.0828\n",
            "Epoch 19/50\n",
            "550/550 [==============================] - 7s 13ms/step - loss: 3.7311 - accuracy: 0.0880 - val_loss: 3.7462 - val_accuracy: 0.0828\n",
            "Epoch 20/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7311 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 21/50\n",
            "550/550 [==============================] - 6s 12ms/step - loss: 3.7311 - accuracy: 0.0880 - val_loss: 3.7462 - val_accuracy: 0.0828\n",
            "Epoch 22/50\n",
            "550/550 [==============================] - 6s 10ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 23/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 24/50\n",
            "550/550 [==============================] - 8s 14ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 25/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 26/50\n",
            "550/550 [==============================] - 7s 13ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 27/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7460 - val_accuracy: 0.0828\n",
            "Epoch 28/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7460 - val_accuracy: 0.0828\n",
            "Epoch 29/50\n",
            "550/550 [==============================] - 7s 12ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7460 - val_accuracy: 0.0828\n",
            "Epoch 30/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7460 - val_accuracy: 0.0828\n",
            "Epoch 31/50\n",
            "550/550 [==============================] - 7s 12ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7460 - val_accuracy: 0.0828\n",
            "Epoch 32/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7460 - val_accuracy: 0.0828\n",
            "Epoch 33/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 34/50\n",
            "550/550 [==============================] - 6s 12ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 35/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7460 - val_accuracy: 0.0828\n",
            "Epoch 36/50\n",
            "550/550 [==============================] - 6s 12ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 37/50\n",
            "550/550 [==============================] - 6s 10ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7460 - val_accuracy: 0.0828\n",
            "Epoch 38/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 39/50\n",
            "550/550 [==============================] - 7s 13ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 40/50\n",
            "550/550 [==============================] - 5s 10ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 41/50\n",
            "550/550 [==============================] - 6s 12ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 42/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 43/50\n",
            "550/550 [==============================] - 5s 10ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 44/50\n",
            "550/550 [==============================] - 7s 13ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 45/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 46/50\n",
            "550/550 [==============================] - 6s 12ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 47/50\n",
            "550/550 [==============================] - 5s 10ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 48/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7460 - val_accuracy: 0.0828\n",
            "Epoch 49/50\n",
            "550/550 [==============================] - 7s 13ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "Epoch 50/50\n",
            "550/550 [==============================] - 5s 9ms/step - loss: 3.7310 - accuracy: 0.0880 - val_loss: 3.7461 - val_accuracy: 0.0828\n",
            "138/138 [==============================] - 1s 4ms/step\n",
            "Predicted Labels: [0 0 0 ... 0 0 0]\n",
            "True Labels: [30 16 12 ... 26  9 40]\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 3.7461 - accuracy: 0.0828\n",
            "Test Loss: 3.7461178302764893\n",
            "Test Accuracy: 0.0827648937702179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######### MLP Architecture #########\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/copy_data19July.csv')\n",
        "\n",
        "# Preprocessing steps\n",
        "def preprocess_data(data):\n",
        "    # Drop unnecessary columns\n",
        "    data = data.drop(['Password', 'Email'], axis=1)\n",
        "\n",
        "    # Preprocess Hold Times column\n",
        "    hold_times_max = data['Hold Times'].apply(lambda x: np.max(eval(x)))\n",
        "    hold_times_min = data['Hold Times'].apply(lambda x: np.min(eval(x)))\n",
        "    hold_times_mean = data['Hold Times'].apply(lambda x: np.mean(eval(x)))\n",
        "\n",
        "    # Preprocess Flight Times column\n",
        "    flight_times_max = data['Flight Times'].apply(lambda x: np.max(eval(x)))\n",
        "    flight_times_min = data['Flight Times'].apply(lambda x: np.min(eval(x)))\n",
        "    flight_times_mean = data['Flight Times'].apply(lambda x: np.mean(eval(x)))\n",
        "\n",
        "    # Preprocess Press/Release Timings column\n",
        "    press_release_max = data['Press/Release Timings'].apply(lambda x: np.max([val[0] for val in eval(x)]))\n",
        "    press_release_min = data['Press/Release Timings'].apply(lambda x: np.min([val[0] for val in eval(x)]))\n",
        "    press_release_mean = data['Press/Release Timings'].apply(lambda x: np.mean([val[0] for val in eval(x)]))\n",
        "\n",
        "    # Preprocess Key Combinations column\n",
        "    key_combinations_length = data['Key Combinations'].apply(lambda x: len(eval(x)))\n",
        "\n",
        "    # Create new preprocessed columns\n",
        "    data['Hold Times_Max'] = hold_times_max\n",
        "    data['Hold Times_Min'] = hold_times_min\n",
        "    data['Hold Times_Mean'] = hold_times_mean\n",
        "    data['Flight Times_Max'] = flight_times_max\n",
        "    data['Flight Times_Min'] = flight_times_min\n",
        "    data['Flight Times_Mean'] = flight_times_mean\n",
        "    data['Press/Release Timings_Max'] = press_release_max\n",
        "    data['Press/Release Timings_Min'] = press_release_min\n",
        "    data['Press/Release Timings_Mean'] = press_release_mean\n",
        "    data['Key Combinations_Length'] = key_combinations_length\n",
        "\n",
        "    # Drop original columns\n",
        "    data = data.drop(['Hold Times', 'Flight Times', 'Press/Release Timings', 'Key Combinations'], axis=1)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply preprocessing to the data\n",
        "preprocessed_data = preprocess_data(data)\n",
        "\n",
        "# Define features and labels\n",
        "label_encoder = LabelEncoder()\n",
        "target = preprocessed_data['ID']\n",
        "target = label_encoder.fit_transform(target)\n",
        "\n",
        "# Split the data into Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data.drop('ID', axis=1), target, test_size=0.2, random_state=0)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build and train Multi-Layer Perceptron(MLP) model\n",
        "# Build and train Multi-Layer Perceptron(MLP) model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
        "\n",
        "# Test the model\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Accuracy: %.2f' % (accuracy * 100))\n",
        "\n",
        "# Test the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int).flatten()  # Convert predicted probabilities to binary labels (0 or 1)\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Convert binary labels back to original labels\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int).flatten()  # Convert predicted probabilities to binary labels (0 or 1)\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1-score (for binary classification)\n",
        "precision = precision_score(y_test, y_pred_binary, average='micro')\n",
        "recall = recall_score(y_test, y_pred_binary, average='micro')\n",
        "f1 = f1_score(y_test, y_pred_binary, average='micro')\n",
        "\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1-score:', f1)\n",
        "\n",
        "# Display confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqKU5aclYc3L",
        "outputId": "5c80ba24-5249-420f-8fa6-c57540c70083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1759/1759 [==============================] - 5s 2ms/step - loss: -18888.5742 - accuracy: 0.0316\n",
            "Epoch 2/10\n",
            "1759/1759 [==============================] - 3s 2ms/step - loss: -227615.6250 - accuracy: 0.0314\n",
            "Epoch 3/10\n",
            "1759/1759 [==============================] - 3s 2ms/step - loss: -795589.7500 - accuracy: 0.0314\n",
            "Epoch 4/10\n",
            "1759/1759 [==============================] - 4s 2ms/step - loss: -1819051.3750 - accuracy: 0.0314\n",
            "Epoch 5/10\n",
            "1759/1759 [==============================] - 5s 3ms/step - loss: -3348188.7500 - accuracy: 0.0314\n",
            "Epoch 6/10\n",
            "1759/1759 [==============================] - 3s 2ms/step - loss: -5461498.5000 - accuracy: 0.0314\n",
            "Epoch 7/10\n",
            "1759/1759 [==============================] - 3s 2ms/step - loss: -8242369.5000 - accuracy: 0.0314\n",
            "Epoch 8/10\n",
            "1759/1759 [==============================] - 3s 2ms/step - loss: -11772404.0000 - accuracy: 0.0314\n",
            "Epoch 9/10\n",
            "1759/1759 [==============================] - 4s 2ms/step - loss: -16129258.0000 - accuracy: 0.0314\n",
            "Epoch 10/10\n",
            "1759/1759 [==============================] - 3s 2ms/step - loss: -21350880.0000 - accuracy: 0.0314\n",
            "138/138 [==============================] - 0s 1ms/step - loss: -24402066.0000 - accuracy: 0.0318\n",
            "Accuracy: 3.18\n",
            "138/138 [==============================] - 0s 1ms/step\n",
            "138/138 [==============================] - 0s 1ms/step\n",
            "Accuracy: 0.031832651205093224\n",
            "Precision: 0.031832651205093224\n",
            "Recall: 0.031832651205093224\n",
            "F1-score: 0.031832651205093224\n",
            "Confusion Matrix:\n",
            "[[  0 394   0 ...   0   0   0]\n",
            " [  0 140   0 ...   0   0   0]\n",
            " [  0  43   0 ...   0   0   0]\n",
            " ...\n",
            " [  0  72   0 ...   0   0   0]\n",
            " [  0  51   0 ...   0   0   0]\n",
            " [  0  67   0 ...   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/copy_data19July.csv')\n",
        "\n",
        "# Get the number of unique users\n",
        "unique_users = data['Email'].nunique()\n",
        "\n",
        "print(\"Number of unique users:\", unique_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FofJfG1ra0G",
        "outputId": "bd017c22-faad-4cc6-bd52-a357c8e12ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique users: 68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### SCNN ARCHITECTURE ######\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/copy_data19July.csv')\n",
        "\n",
        "# Extract features from 'Hold Times' column\n",
        "data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "\n",
        "# Extract features from 'Hold Times' column\n",
        "data['Hold Times_Max'] = data['Hold Times'].apply(lambda x: np.max([float(val) for val in x]))\n",
        "data['Hold Times_Min'] = data['Hold Times'].apply(lambda x: np.min([float(val) for val in x]))\n",
        "data['Hold Times_Mean'] = data['Hold Times'].apply(lambda x: np.mean([float(val) for val in x]))\n",
        "data['Hold Times_Std'] = data['Hold Times'].apply(lambda x: np.std([float(val) for val in x]))\n",
        "data['Hold Times_Sum'] = data['Hold Times'].apply(lambda x: np.sum([float(val) for val in x]))\n",
        "data['Hold Times_Length'] = data['Hold Times'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# Extract features from 'Flight Times' column\n",
        "data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "\n",
        "# Extract features from 'Flight Times' column\n",
        "data['Flight Times_Max'] = data['Flight Times'].apply(lambda x: np.max([float(val) for val in x]))\n",
        "data['Flight Times_Min'] = data['Flight Times'].apply(lambda x: np.min([float(val) for val in x]))\n",
        "data['Flight Times_Mean'] = data['Flight Times'].apply(lambda x: np.mean([float(val) for val in x]))\n",
        "data['Flight Times_Std'] = data['Flight Times'].apply(lambda x: np.std([float(val) for val in x]))\n",
        "data['Flight Times_Sum'] = data['Flight Times'].apply(lambda x: np.sum([float(val) for val in x]))\n",
        "data['Flight Times_Length'] = data['Flight Times'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# Extract features from 'Press/Release Timings' column\n",
        "data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "\n",
        "# Extract features from 'Press/Release Timings' column\n",
        "data['Press/Release Timings_Max'] = data['Press/Release Timings'].apply(lambda x: np.max([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Min'] = data['Press/Release Timings'].apply(lambda x: np.min([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Mean'] = data['Press/Release Timings'].apply(lambda x: np.mean([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Std'] = data['Press/Release Timings'].apply(lambda x: np.std([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Sum'] = data['Press/Release Timings'].apply(lambda x: np.sum([float(val[0]) for val in x]))\n",
        "data['Press/Release Timings_Length'] = data['Press/Release Timings'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# Extract features from 'Key Combinations' column\n",
        "data['Key Combinations_Unique_Count'] = data['Key Combinations'].apply(lambda x: len(set(x)))\n",
        "data['Key Combinations_Length'] = data['Key Combinations'].apply(lambda x: len(x))\n",
        "\n",
        "# Display the updated dataset with extracted features\n",
        "# print(data)\n",
        "\n",
        "\n",
        "#PreProcessing it\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/copy_data19July.csv')\n",
        "\n",
        "# Preprocessing steps\n",
        "# Preprocessing steps\n",
        "def preprocess_data(data):\n",
        "    # Drop unnecessary columns\n",
        "    data = data.drop(['Password', 'Email'], axis=1)\n",
        "\n",
        "    # Preprocess Hold Times column\n",
        "    hold_times_max = data['Hold Times'].apply(lambda x: np.max(eval(x)))\n",
        "    hold_times_min = data['Hold Times'].apply(lambda x: np.min(eval(x)))\n",
        "    hold_times_mean = data['Hold Times'].apply(lambda x: np.mean(eval(x)))\n",
        "\n",
        "    # Preprocess Flight Times column\n",
        "    flight_times_max = data['Flight Times'].apply(lambda x: np.max(eval(x)))\n",
        "    flight_times_min = data['Flight Times'].apply(lambda x: np.min(eval(x)))\n",
        "    flight_times_mean = data['Flight Times'].apply(lambda x: np.mean(eval(x)))\n",
        "\n",
        "    # Preprocess Press/Release Timings column\n",
        "    press_release_max = data['Press/Release Timings'].apply(lambda x: np.max([val[0] for val in eval(x)]))\n",
        "    press_release_min = data['Press/Release Timings'].apply(lambda x: np.min([val[0] for val in eval(x)]))\n",
        "    press_release_mean = data['Press/Release Timings'].apply(lambda x: np.mean([val[0] for val in eval(x)]))\n",
        "\n",
        "    # Create new preprocessed columns\n",
        "    data['Hold Times_Max'] = hold_times_max\n",
        "    data['Hold Times_Min'] = hold_times_min\n",
        "    data['Hold Times_Mean'] = hold_times_mean\n",
        "    data['Flight Times_Max'] = flight_times_max\n",
        "    data['Flight Times_Min'] = flight_times_min\n",
        "    data['Flight Times_Mean'] = flight_times_mean\n",
        "    data['Press/Release Timings_Max'] = press_release_max\n",
        "    data['Press/Release Timings_Min'] = press_release_min\n",
        "    data['Press/Release Timings_Mean'] = press_release_mean\n",
        "\n",
        "    # Drop original columns\n",
        "    data = data.drop(['Hold Times', 'Flight Times', 'Press/Release Timings'], axis=1)\n",
        "\n",
        "    # Scale numerical columns\n",
        "    numerical_columns = ['Total Hold Time', 'Total Flight Time', 'Hold Times_Max',\n",
        "                         'Hold Times_Min', 'Hold Times_Mean', 'Flight Times_Max', 'Flight Times_Min',\n",
        "                         'Flight Times_Mean', 'Press/Release Timings_Max', 'Press/Release Timings_Min',\n",
        "                         'Press/Release Timings_Mean']\n",
        "    scaler = StandardScaler()\n",
        "    data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "# Apply preprocessing to the data\n",
        "preprocessed_data = preprocess_data(data)\n",
        "\n",
        "# Models and evaluating it\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models, backend as K\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "# Convert the preprocessed data to numpy arrays\n",
        "X = preprocessed_data.values\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "y = preprocessed_data['ID']  # Initialize the target as zeros for a typical binary classification\n",
        "y_encoded = pd.get_dummies(y)\n",
        "\n",
        "# Convert target labels to numpy arrays\n",
        "y = preprocessed_data['ID']  # Initialize the target as zeros for a typical binary classification\n",
        "y_encoded = pd.get_dummies(y)\n",
        "\n",
        "# Split the data into training and testing sets with correct target labels\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify columns with string values (non-numerical) in X_train and X_test\n",
        "# Identify columns with string values (non-numerical) in X_train and X_test\n",
        "string_columns_train = X_train[:, [isinstance(val, str) for val in X_train[0]]]\n",
        "string_columns_test = X_test[:, [isinstance(val, str) for val in X_test[0]]]\n",
        "\n",
        "# Identify columns with numerical values in X_train and X_test\n",
        "numerical_columns_train = X_train[:, [isinstance(val, (int, float)) for val in X_train[0]]]\n",
        "numerical_columns_test = X_test[:, [isinstance(val, (int, float)) for val in X_test[0]]]\n",
        "\n",
        "# Convert numerical columns to numpy array with appropriate data type\n",
        "numerical_columns_train = np.array(numerical_columns_train, dtype=np.float32)\n",
        "numerical_columns_test = np.array(numerical_columns_test, dtype=np.float32)\n",
        "\n",
        "# Normalize only numerical columns\n",
        "numerical_columns_train = (numerical_columns_train - np.mean(numerical_columns_train, axis=0)) / np.std(numerical_columns_train, axis=0)\n",
        "numerical_columns_test = (numerical_columns_test - np.mean(numerical_columns_train, axis=0)) / np.std(numerical_columns_train, axis=0)\n",
        "\n",
        "# Perform one-hot encoding on the string columns\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "string_columns_train_encoded = encoder.fit_transform(string_columns_train).toarray()\n",
        "string_columns_test_encoded = encoder.transform(string_columns_test).toarray()\n",
        "\n",
        "# Concatenate back numerical and one-hot encoded string columns for X_train and X_test\n",
        "X_train = np.concatenate((numerical_columns_train, string_columns_train_encoded), axis=1)\n",
        "X_test = np.concatenate((numerical_columns_test, string_columns_test_encoded), axis=1)\n",
        "\n",
        "# Define Siamese CNN model\n",
        "input_shape = (X_train.shape[1],)\n",
        "input_left = tf.keras.layers.Input(shape=input_shape)\n",
        "input_right = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "siamese_cnn = models.Sequential([\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "\n",
        "\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "\n",
        "\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "\n",
        "\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "\n",
        "\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "\n",
        "\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "])\n",
        "\n",
        "encoded_left = siamese_cnn(input_left)\n",
        "encoded_right = siamese_cnn(input_right)\n",
        "\n",
        "# Calculate the Euclidean distance between the two encoded inputs\n",
        "distance = layers.Subtract()([encoded_left, encoded_right])\n",
        "distance = layers.Lambda(lambda x: K.abs(x))(distance) # Corrected line for calculating the absolute value\n",
        "\n",
        "# Output layer\n",
        "num_classes = len(y_encoded.columns)\n",
        "output = layers.Dense(num_classes, activation='softmax')(distance)\n",
        "\n",
        "# Siamese model\n",
        "siamese_model = models.Model(inputs=[input_left, input_right], outputs=output)\n",
        "# sgd = SGD(learning_rate=0.01)\n",
        "# Compile and train Siamese CNN model\n",
        "siamese_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "siamese_model.fit([X_train, X_train], y_train, epochs=10, batch_size=32, validation_data=([X_test, X_test], y_test))\n",
        "\n",
        "y_pred = siamese_model.predict([X_test, X_test])\n",
        "\n",
        "# Since it's a binary classification problem, convert probabilities to binary predictions (0 or 1)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Get the true labels\n",
        "y_true_labels = np.argmax(y_test.values, axis=1)\n",
        "\n",
        "# Print the predicted labels and the true labels\n",
        "print(\"Predicted Labels:\", y_pred_labels)\n",
        "print(\"True Labels:\", y_true_labels)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = siamese_model.evaluate([X_test, X_test], y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBmTS0NhtCFL",
        "outputId": "87044c2e-0768-4e7a-9071-ff4615808c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "550/550 [==============================] - 26s 36ms/step - loss: 2.2163 - accuracy: 0.3956 - val_loss: 3.9523 - val_accuracy: 0.0252\n",
            "Epoch 2/10\n",
            "550/550 [==============================] - 17s 31ms/step - loss: 0.9459 - accuracy: 0.7064 - val_loss: 3.9529 - val_accuracy: 0.0252\n",
            "Epoch 3/10\n",
            "550/550 [==============================] - 18s 33ms/step - loss: 0.3880 - accuracy: 0.8799 - val_loss: 3.9529 - val_accuracy: 0.0252\n",
            "Epoch 4/10\n",
            "550/550 [==============================] - 17s 31ms/step - loss: 0.1977 - accuracy: 0.9371 - val_loss: 3.9533 - val_accuracy: 0.0189\n",
            "Epoch 5/10\n",
            "550/550 [==============================] - 17s 31ms/step - loss: 0.1383 - accuracy: 0.9576 - val_loss: 3.9532 - val_accuracy: 0.0252\n",
            "Epoch 6/10\n",
            "550/550 [==============================] - 17s 31ms/step - loss: 0.0899 - accuracy: 0.9725 - val_loss: 3.9533 - val_accuracy: 0.0123\n",
            "Epoch 7/10\n",
            "550/550 [==============================] - 17s 31ms/step - loss: 0.0792 - accuracy: 0.9732 - val_loss: 3.9531 - val_accuracy: 0.0123\n",
            "Epoch 8/10\n",
            "550/550 [==============================] - 18s 32ms/step - loss: 0.0929 - accuracy: 0.9709 - val_loss: 3.9533 - val_accuracy: 9.0950e-04\n",
            "Epoch 9/10\n",
            "550/550 [==============================] - 16s 30ms/step - loss: 0.0834 - accuracy: 0.9732 - val_loss: 3.9536 - val_accuracy: 0.0189\n",
            "Epoch 10/10\n",
            "550/550 [==============================] - 17s 31ms/step - loss: 0.0775 - accuracy: 0.9787 - val_loss: 3.9541 - val_accuracy: 0.0189\n",
            "138/138 [==============================] - 1s 6ms/step\n",
            "Predicted Labels: [17 17 17 ... 17 17 17]\n",
            "True Labels: [30 16 12 ... 26  9 40]\n",
            "138/138 [==============================] - 1s 6ms/step - loss: 3.9541 - accuracy: 0.0189\n",
            "Test Loss: 3.9540772438049316\n",
            "Test Accuracy: 0.01887221448123455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from ast import literal_eval\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/copy_data19July.csv')\n",
        "\n",
        "# Preprocessing steps\n",
        "def preprocess_data(data):\n",
        "    # Extract features from 'Hold Times' column\n",
        "    data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "    data['Hold Times_Max'] = data['Hold Times'].apply(lambda x: np.max([float(val) for val in x]))\n",
        "    data['Hold Times_Min'] = data['Hold Times'].apply(lambda x: np.min([float(val) for val in x]))\n",
        "    data['Hold Times_Mean'] = data['Hold Times'].apply(lambda x: np.mean([float(val) for val in x]))\n",
        "\n",
        "    # Extract features from 'Flight Times' column\n",
        "    data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "    data['Flight Times_Max'] = data['Flight Times'].apply(lambda x: np.max([float(val) for val in x]))\n",
        "    data['Flight Times_Min'] = data['Flight Times'].apply(lambda x: np.min([float(val) for val in x]))\n",
        "    data['Flight Times_Mean'] = data['Flight Times'].apply(lambda x: np.mean([float(val) for val in x]))\n",
        "\n",
        "    # Extract features from 'Press/Release Timings' column\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "    data['Press/Release Timings_Max'] = data['Press/Release Timings'].apply(lambda x: np.max([float(val[0]) for val in x]))\n",
        "    data['Press/Release Timings_Min'] = data['Press/Release Timings'].apply(lambda x: np.min([float(val[0]) for val in x]))\n",
        "    data['Press/Release Timings_Mean'] = data['Press/Release Timings'].apply(lambda x: np.mean([float(val[0]) for val in x]))\n",
        "\n",
        "    # Extract features from 'Key Combinations' column\n",
        "    data['Key Combinations_Unique_Count'] = data['Key Combinations'].apply(lambda x: len(set(x)))\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    data = data.drop(['Password', 'Email', 'Hold Times', 'Flight Times', 'Press/Release Timings'], axis=1)\n",
        "\n",
        "    # Separate numerical and categorical columns\n",
        "    numerical_columns = data.select_dtypes(include=[np.number]).columns\n",
        "    categorical_columns = data.select_dtypes(include=[object]).columns\n",
        "\n",
        "    # Preprocess numerical columns\n",
        "    scaler = StandardScaler()\n",
        "    data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
        "\n",
        "    # One-hot encode categorical columns\n",
        "    data = pd.get_dummies(data, columns=categorical_columns)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply preprocessing to the data\n",
        "preprocessed_data = preprocess_data(data)\n",
        "# Custom data augmentation function for tabular data\n",
        "def data_augmentation(X_train, y_train, num_augmentations=5, noise_factor=0.01):\n",
        "    X_train_augmented = []\n",
        "    y_train_augmented = []\n",
        "\n",
        "    for i in range(num_augmentations):\n",
        "        # Add noise to the training data\n",
        "        noise = np.random.normal(loc=0, scale=noise_factor, size=X_train.shape)\n",
        "        X_train_augmented.append(X_train + noise)\n",
        "        y_train_augmented.append(y_train)\n",
        "\n",
        "    # Concatenate the augmented data along the first axis to form one array\n",
        "    X_train_augmented = np.concatenate(X_train_augmented, axis=0)\n",
        "    y_train_augmented = np.concatenate(y_train_augmented, axis=0)\n",
        "\n",
        "    return X_train_augmented, y_train_augmented\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = preprocessed_data.drop('ID', axis=1).values\n",
        "y = preprocessed_data['ID']\n",
        "y_encoded = pd.get_dummies(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply data augmentation to the training data\n",
        "num_augmentations = 5\n",
        "noise_factor = 0.01\n",
        "X_train_augmented, y_train_augmented = data_augmentation(X_train, y_train, num_augmentations, noise_factor)\n",
        "\n",
        "\n",
        "# Models and evaluating it\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models, backend as K\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Convert the preprocessed data to numpy arrays\n",
        "X = preprocessed_data.drop('ID', axis=1).values\n",
        "y = preprocessed_data['ID']\n",
        "y_encoded = pd.get_dummies(y)\n",
        "\n",
        "# Split the data into training and testing sets with correct target labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply data augmentation to the training data (step 2)\n",
        "num_augmentations = 5\n",
        "noise_factor = 0.01\n",
        "X_train_augmented, y_train_augmented = data_augmentation(X_train, y_train, num_augmentations, noise_factor)\n",
        "\n",
        "\n",
        "\n",
        "# Define Siamese CNN model\n",
        "input_shape = (X_train.shape[1],)\n",
        "input_left = layers.Input(shape=input_shape)\n",
        "input_right = layers.Input(shape=input_shape)\n",
        "\n",
        "siamese_cnn = models.Sequential([\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "])\n",
        "\n",
        "encoded_left = siamese_cnn(input_left)\n",
        "encoded_right = siamese_cnn(input_right)\n",
        "\n",
        "# Calculate the Euclidean distance between the two encoded inputs\n",
        "distance = layers.Subtract()([encoded_left, encoded_right])\n",
        "distance = layers.Lambda(lambda x: K.abs(x))(distance)  # Corrected line for calculating the absolute value\n",
        "\n",
        "# Output layer\n",
        "num_classes = len(y_encoded.columns)\n",
        "output = layers.Dense(num_classes, activation='softmax')(distance)\n",
        "\n",
        "# Siamese model\n",
        "siamese_model = models.Model(inputs=[input_left, input_right], outputs=output)\n",
        "\n",
        "# Compile and train Siamese CNN model\n",
        "siamese_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "siamese_model.fit([X_train_augmented, X_train_augmented], y_train_augmented, epochs=10, batch_size=32, validation_data=([X_test, X_test], y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = siamese_model.evaluate([X_test, X_test], y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQQ9DZmWws20",
        "outputId": "b32b1a22-a71b-4f00-bea6-8b13712dbeaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2749/2749 [==============================] - 88s 30ms/step - loss: 0.9411 - accuracy: 0.7378 - val_loss: 3.9530 - val_accuracy: 0.0236\n",
            "Epoch 2/10\n",
            "2749/2749 [==============================] - 85s 31ms/step - loss: 0.1045 - accuracy: 0.9675 - val_loss: 3.9546 - val_accuracy: 0.0236\n",
            "Epoch 3/10\n",
            "2749/2749 [==============================] - 82s 30ms/step - loss: 0.0792 - accuracy: 0.9776 - val_loss: 3.9558 - val_accuracy: 0.0236\n",
            "Epoch 4/10\n",
            "2749/2749 [==============================] - 87s 32ms/step - loss: 0.0586 - accuracy: 0.9836 - val_loss: 3.9571 - val_accuracy: 0.0236\n",
            "Epoch 5/10\n",
            "2749/2749 [==============================] - 85s 31ms/step - loss: 0.0554 - accuracy: 0.9849 - val_loss: 3.9564 - val_accuracy: 0.0236\n",
            "Epoch 6/10\n",
            "2749/2749 [==============================] - 84s 31ms/step - loss: 0.0495 - accuracy: 0.9872 - val_loss: 3.9557 - val_accuracy: 0.0236\n",
            "Epoch 7/10\n",
            "2749/2749 [==============================] - 83s 30ms/step - loss: 0.0438 - accuracy: 0.9888 - val_loss: 3.9572 - val_accuracy: 0.0236\n",
            "Epoch 8/10\n",
            "2749/2749 [==============================] - 87s 32ms/step - loss: 0.0391 - accuracy: 0.9897 - val_loss: 3.9593 - val_accuracy: 0.0236\n",
            "Epoch 9/10\n",
            "2749/2749 [==============================] - 86s 31ms/step - loss: 0.0362 - accuracy: 0.9908 - val_loss: 3.9587 - val_accuracy: 0.0236\n",
            "Epoch 10/10\n",
            "2749/2749 [==============================] - 88s 32ms/step - loss: 0.0368 - accuracy: 0.9908 - val_loss: 3.9596 - val_accuracy: 0.0236\n",
            "138/138 [==============================] - 1s 6ms/step - loss: 3.9596 - accuracy: 0.0236\n",
            "Test Loss: 3.9595870971679688\n",
            "Test Accuracy: 0.023647112771868706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset (replace 'your_dataset.csv' with the actual path to your CSV file)\n",
        "data = pd.read_csv('/content/drive/MyDrive/custom_data24thJuly.csv')\n",
        "\n",
        "\n",
        "# Drop duplicate rows and keep only unique rows\n",
        "unique_rows = data.drop_duplicates('Email')\n",
        "\n",
        "# Display the unique rows\n",
        "print(unique_rows)\n"
      ],
      "metadata": {
        "id": "9-jSCTw_wt2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6efac19-381c-41a5-c5e0-94dbf1a8686a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      ID     Password                          Email  \\\n",
            "0      1  Windsor@123  6ganesanparamasivam@gmail.com   \n",
            "9      1  Windsor@123                abdul@gmail.com   \n",
            "23     1  Windsor@123        abiramiviji99@gmail.com   \n",
            "66     1  Windsor@123               ade1@uwindsor.ca   \n",
            "80     1  Windsor@123               akshat@gmail.com   \n",
            "...   ..          ...                            ...   \n",
            "1275  75  Windsor@123        umaselvaraj70@gmail.com   \n",
            "1290  78  Windsor@123            vagdoda@uwindsor.ca   \n",
            "1300  75  Windsor@123              vijay@uwindsor.ca   \n",
            "1306  64  Windsor@123        vijayraghavan@gmail.com   \n",
            "1318  64  Windsor@123              zewbare@gmail.com   \n",
            "\n",
            "                                             Hold Times  \\\n",
            "0     [63, 88, 127, 11, 49, 199, 69, 156, 92, 138, 1...   \n",
            "9     [63, 70, 121, 3, 55, 185, 71, 133, 87, 47, 139...   \n",
            "23    [98, 16, 73, 40, 75, 21, 72, 15, 106, 30, 61, ...   \n",
            "66    [94, 712, 89, 0, 79, 24, 88, 16, 109, 1, 90, 4...   \n",
            "80    [96, 45, 21, 46, 86, 6, 104, 44, 116, 56, 41, ...   \n",
            "...                                                 ...   \n",
            "1275  [68, 271, 72, 361, 82, 43, 50, 454, 72, 224, 4...   \n",
            "1290  [142, 70, 158, 10, 139, 373, 129, 63, 165, 23,...   \n",
            "1300  [66, 240, 79, 785, 79, 96, 48, 461, 69, 182, 8...   \n",
            "1306  [142, 28, 172, 6, 98, 19, 151, 845, 177, 2, 10...   \n",
            "1318  [107, 103, 71, 547, 130, 613, 133, 119, 84, 27...   \n",
            "\n",
            "                                           Flight Times  \\\n",
            "0     [63, 127, 49, 69, 92, 138, 43, 59, 123, 64, 63...   \n",
            "9     [63, 3, 71, 87, 139, 52, 64, 57, 69, 59, 39, 7...   \n",
            "23    [16, 40, 21, 30, 111, 32, 117, 31, 28, 102, 4,...   \n",
            "66    [94, 89, 24, 16, 40, 48, 32, 81, 70, 101, 80, ...   \n",
            "80    [45, 46, 104, 56, 109, 116, 36, 80, 91, 51, 99...   \n",
            "...                                                 ...   \n",
            "1275  [68, 72, 82, 50, 72, 47, 73, 86, 72, 64, 90, 6...   \n",
            "1290  [142, 158, 139, 129, 165, 154, 568, 6, 30, 69,...   \n",
            "1300  [66, 79, 79, 48, 69, 89, 90, 121, 71, 74, 93, ...   \n",
            "1306  [142, 172, 98, 151, 177, 109, 102, 100, 138, 8...   \n",
            "1318  [107, 71, 130, 133, 84, 103, 83, 81, 73, 103, ...   \n",
            "\n",
            "                                  Press/Release Timings  \\\n",
            "0     [(0, 63), (151, 278), (289, 338), (537, 606), ...   \n",
            "9     [(0, 63), (254, 257), (497, 568), (701, 788), ...   \n",
            "23    [(98, 114), (187, 227), (302, 323), (516, 546)...   \n",
            "66    [(0, 94), (806, 895), (974, 998), (1086, 1102)...   \n",
            "80    [(96, 141), (162, 208), (300, 404), (564, 620)...   \n",
            "...                                                 ...   \n",
            "1275  [(0, 68), (339, 411), (772, 854), (897, 947), ...   \n",
            "1290  [(0, 142), (212, 370), (380, 519), (892, 1021)...   \n",
            "1300  [(0, 66), (306, 385), (1170, 1249), (1345, 139...   \n",
            "1306  [(0, 142), (170, 342), (348, 446), (465, 616),...   \n",
            "1318  [(1, 108), (211, 282), (829, 959), (1572, 1705...   \n",
            "\n",
            "                                       Key Combinations  Total Hold Time  \\\n",
            "0     [('d', 'd'), ('a', 'a'), ('r', 'r'), ('u', 'u'...             8835   \n",
            "9     [('d', 'd'), ('r', 'a'), ('u', 'u'), ('n', 'n'...             8022   \n",
            "23    [('y', 'r'), ('u', 'y'), ('g', 'u'), ('k', 'a'...             5762   \n",
            "66    [('Tab', 'Tab'), ('r', 'r'), ('u', 'y'), ('g',...             6766   \n",
            "80    [('y', 'r'), ('u', 'y'), ('g', 'g'), ('k', 'a'...             7996   \n",
            "...                                                 ...              ...   \n",
            "1275  [('m', 'm'), ('y', 'y'), ('t', 't'), ('h', 'h'...            16967   \n",
            "1290  [('v', 'v'), ('i', 'i'), ('s', 's'), ('h', 'h'...            12283   \n",
            "1300  [('m', 'm'), ('y', 'y'), ('t', 't'), ('h', 'h'...            16647   \n",
            "1306  [('d', 'd'), ('a', 'a'), ('v', 'v'), ('e', 'e'...            19315   \n",
            "1318  [('s', 's'), ('u', 'u'), ('g', 'g'), ('a', 'a'...            21491   \n",
            "\n",
            "      Total Flight Time  Total Key Combinations  \n",
            "0                  3111                      32  \n",
            "9                  2658                      30  \n",
            "23                 1402                      27  \n",
            "66                 2191                      31  \n",
            "80                 2445                      39  \n",
            "...                 ...                     ...  \n",
            "1275               2417                      32  \n",
            "1290               4384                      35  \n",
            "1300               2533                      32  \n",
            "1306               4095                      31  \n",
            "1318               2732                      32  \n",
            "\n",
            "[92 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the dataframe\n",
        "df = pd.read_csv('/content/drive/MyDrive/custom_data24thJuly.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "df['Hold Times'] = df['Hold Times'].apply(len)\n",
        "df['Flight Times'] = df['Flight Times'].apply(len)\n",
        "df['Press/Release Timings'] = df['Press/Release Timings'].apply(len)\n",
        "df['Key Combinations'] = df['Key Combinations'].apply(len)\n",
        "\n",
        "# Define the features and the target\n",
        "X = df.drop(['ID', 'Email', 'Password'], axis=1) # Drop 'Password' column\n",
        "y_id = df['ID']\n",
        "y_email = df['Email']\n",
        "\n",
        "# Scale the features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train, test, split for 'ID'\n",
        "X_train_id, X_test_id, y_train_id, y_test_id = train_test_split(X, y_id, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the model for 'ID'\n",
        "clf_id = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the model for 'ID'\n",
        "clf_id.fit(X_train_id, y_train_id)\n",
        "\n",
        "# Make predictions for 'ID' and evaluate\n",
        "y_pred_id = clf_id.predict(X_test_id)\n",
        "print(\"Classification Report for 'ID':\\n\", classification_report(y_test_id, y_pred_id))\n",
        "\n",
        "# Train, test, split for 'Email'\n",
        "X_train_email, X_test_email, y_train_email, y_test_email = train_test_split(X, y_email, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the model for 'Email'\n",
        "clf_email = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the model for 'Email'\n",
        "clf_email.fit(X_train_email, y_train_email)\n",
        "\n",
        "# Make predictions for 'Email' and evaluate\n",
        "y_pred_email = clf_email.predict(X_test_email)\n",
        "print(\"Classification Report for 'Email':\\n\", classification_report(y_test_email, y_pred_email))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl71kGWtNKp_",
        "outputId": "8f5a9c02-03f3-494d-8555-604e2b07a7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 'ID':\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.75      0.95      0.84        19\n",
            "           2       0.90      1.00      0.95         9\n",
            "           3       0.50      1.00      0.67         1\n",
            "           4       1.00      1.00      1.00         2\n",
            "           5       1.00      1.00      1.00         3\n",
            "           6       0.70      1.00      0.82         7\n",
            "           7       0.71      1.00      0.83         5\n",
            "           8       0.75      0.60      0.67        10\n",
            "           9       0.00      0.00      0.00         4\n",
            "          10       0.40      0.80      0.53         5\n",
            "          11       1.00      0.67      0.80         3\n",
            "          12       1.00      0.50      0.67         6\n",
            "          13       0.00      0.00      0.00         1\n",
            "          14       1.00      0.75      0.86         8\n",
            "          15       1.00      0.33      0.50         6\n",
            "          16       0.25      0.50      0.33         2\n",
            "          17       0.88      0.64      0.74        11\n",
            "          18       0.00      0.00      0.00         1\n",
            "          19       0.50      1.00      0.67         1\n",
            "          20       1.00      1.00      1.00         3\n",
            "          21       0.67      1.00      0.80         2\n",
            "          22       0.00      0.00      0.00         0\n",
            "          23       0.67      1.00      0.80         2\n",
            "          25       1.00      1.00      1.00         1\n",
            "          26       0.75      1.00      0.86         6\n",
            "          27       0.67      0.75      0.71         8\n",
            "          28       0.82      1.00      0.90         9\n",
            "          29       1.00      1.00      1.00        12\n",
            "          30       1.00      0.67      0.80         6\n",
            "          31       1.00      0.71      0.83         7\n",
            "          32       0.73      1.00      0.85        11\n",
            "          33       1.00      0.75      0.86         4\n",
            "          34       1.00      0.83      0.91         6\n",
            "          35       0.75      0.60      0.67         5\n",
            "          36       0.00      0.00      0.00         0\n",
            "          37       0.60      0.60      0.60         5\n",
            "          38       1.00      0.75      0.86         4\n",
            "          40       1.00      0.33      0.50         3\n",
            "          41       0.80      0.80      0.80         5\n",
            "          44       0.33      0.33      0.33         3\n",
            "          45       0.00      0.00      0.00         4\n",
            "          46       0.75      0.75      0.75         4\n",
            "          47       0.33      1.00      0.50         1\n",
            "          48       1.00      1.00      1.00         1\n",
            "          49       0.50      0.67      0.57         3\n",
            "          51       0.50      0.50      0.50         4\n",
            "          52       0.50      1.00      0.67         1\n",
            "          53       1.00      0.67      0.80         3\n",
            "          54       0.67      1.00      0.80         2\n",
            "          55       0.00      0.00      0.00         1\n",
            "          57       0.00      0.00      0.00         1\n",
            "          58       1.00      1.00      1.00         3\n",
            "          62       0.00      0.00      0.00         1\n",
            "          63       0.00      0.00      0.00         1\n",
            "          64       1.00      0.67      0.80         6\n",
            "          65       1.00      1.00      1.00         2\n",
            "          66       0.67      0.67      0.67         3\n",
            "          67       1.00      1.00      1.00         1\n",
            "          69       1.00      0.50      0.67         4\n",
            "          71       0.33      0.25      0.29         4\n",
            "          72       0.50      1.00      0.67         1\n",
            "          73       1.00      0.50      0.67         2\n",
            "          74       0.50      1.00      0.67         1\n",
            "          75       1.00      0.50      0.67         4\n",
            "          76       0.00      0.00      0.00         1\n",
            "          78       1.00      1.00      1.00         2\n",
            "          79       0.67      0.67      0.67         3\n",
            "          80       0.67      0.67      0.67         3\n",
            "\n",
            "    accuracy                           0.74       273\n",
            "   macro avg       0.66      0.66      0.63       273\n",
            "weighted avg       0.77      0.74      0.73       273\n",
            "\n",
            "Classification Report for 'Email':\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "         Siddhardh@gmail.com       0.00      0.00      0.00         1\n",
            "             abdul@gmail.com       0.00      0.00      0.00         0\n",
            "     abiramiviji99@gmail.com       0.54      0.54      0.54        13\n",
            "            ade1@uwindsor.ca       0.00      0.00      0.00         4\n",
            "            akshat@gmail.com       0.00      0.00      0.00         0\n",
            "    anjaneyaragu75@gmail.com       0.20      0.25      0.22         4\n",
            "            arugk93@gmai.com       0.50      1.00      0.67         2\n",
            "           arugk93@gmail.com       0.33      0.50      0.40         2\n",
            "     arunkrishnavj@gmail.com       0.00      0.00      0.00         1\n",
            "      arunreddy096@gmail.com       0.33      0.25      0.29         4\n",
            "             bahar@gmail.com       0.17      0.50      0.25         2\n",
            "              bnry@gmail.com       0.40      0.67      0.50         3\n",
            "              bnry@gmail.cpm       0.00      0.00      0.00         0\n",
            "       bobmelissa@hotmail.ca       0.00      0.00      0.00         4\n",
            "      chitradevi6u@gmail.com       0.60      0.38      0.46         8\n",
            "        darunaru25@gmail.com       0.00      0.00      0.00         2\n",
            "   dasvanthreddy.k@gmail.com       1.00      0.50      0.67         2\n",
            "         davemaddy@gmail.com       0.57      0.80      0.67         5\n",
            "     devam.another@gmail.com       0.00      0.00      0.00         3\n",
            " dharmakannan_lynk@gmail.com       1.00      1.00      1.00         3\n",
            "             dhruv@gmail.com       0.00      0.00      0.00         2\n",
            "            dinesh@gmail.com       0.00      0.00      0.00         1\n",
            "    fr.xavier.sj01@gmail.com       0.00      0.00      0.00         1\n",
            "                      gallad       0.67      1.00      0.80         2\n",
            "    greeghagreen99@gmail.com       1.00      0.25      0.40         4\n",
            "        haribhav@uwindsor.ca       0.00      0.00      0.00         1\n",
            "      harika.y1997@gmail.com       0.00      0.00      0.00         3\n",
            "        hharsha107@gmail.com       0.50      0.33      0.40         3\n",
            "        imtiaz41@uwindsor.ca       0.20      0.50      0.29         4\n",
            "      irfangowri23@gmail.com       1.00      0.17      0.29         6\n",
            "        jadawalj@uwindsor.ca       0.60      1.00      0.75         3\n",
            "             janki@gmail.com       0.00      0.00      0.00         1\n",
            "     jayagopalan60@gmail.com       0.25      0.50      0.33         2\n",
            "    jayagopalan60@gmaill.com       0.00      0.00      0.00         1\n",
            "        jaymodi123@gmail.com       1.00      0.50      0.67         2\n",
            "        jayshankar@gmail.com       0.50      1.00      0.67         1\n",
            "       josephvijay@gmail.com       0.57      0.80      0.67         5\n",
            " kasinathangovind99@gmai.com       0.00      0.00      0.00         2\n",
            "kasinathangovind99@gmail.com       0.00      0.00      0.00         4\n",
            "   keshruwalaharsh@gmail.com       0.43      0.60      0.50         5\n",
            "        khanna73@uwindsor.ca       0.00      0.00      0.00         7\n",
            " lavanyaadhimoolam@gmail.com       0.27      0.75      0.40         4\n",
            "          mahima@uwindsor.ca       0.67      0.25      0.36         8\n",
            "  mathivanan061264@gmail.com       0.50      0.80      0.62        10\n",
            "       mythiliguna@gmail.com       0.00      0.00      0.00         3\n",
            "      najeemkollam@gmail.com       0.17      0.25      0.20         4\n",
            "     nanditajoseph@gmail.com       0.00      0.00      0.00         1\n",
            "          nehaanto@gmail.com       0.00      0.00      0.00         4\n",
            "             nikki@gmail.com       0.00      0.00      0.00         2\n",
            "  pkalyanvenkatesh@gmail.com       0.50      0.33      0.40         6\n",
            "         poludas@uwindsor.ca       1.00      0.50      0.67         2\n",
            "  poornimavenkat77@gmail.com       0.33      0.33      0.33         3\n",
            "  rajarethinamsiva@gmail.com       0.20      0.25      0.22         4\n",
            "         ramananvs@gmail.com       0.00      0.00      0.00         3\n",
            "       rasul_ramco@gmail.com       0.20      1.00      0.33         1\n",
            "          riddhi99@gmail.com       1.00      0.50      0.67         4\n",
            "           rithvik@gmail.com       1.00      1.00      1.00         4\n",
            "     ryugakirafudo@gmail.com       0.43      0.56      0.49        18\n",
            "    sakuntala.nagu@gmail.com       0.00      0.00      0.00         1\n",
            "        samuel9997@gmail.com       0.00      0.00      0.00         1\n",
            "         samvishal@gmail.com       0.50      1.00      0.67         1\n",
            "samyuktharamesh123@gmail.com       0.50      0.50      0.50         2\n",
            "        sathishv@uwindsor.ca       0.50      0.50      0.50         4\n",
            "   senthilnathan11@gmail.com       0.50      0.67      0.57         3\n",
            "      shahshipra90@gmail.com       0.50      0.67      0.57         3\n",
            "         shalooaru@gmail.com       0.50      0.50      0.50         2\n",
            "     shaonbhatta@uwindsor.ca       1.00      0.17      0.29         6\n",
            "     shoanbhatta@uwindsor.ca       0.00      0.00      0.00         3\n",
            "      shubhamduker@gmail.com       0.00      0.00      0.00         2\n",
            "    solaikayal2020@gmail.com       0.00      0.00      0.00         3\n",
            "  sriramvelaga1205@gmail.com       0.33      0.33      0.33         3\n",
            "            steeve@gmail.com       0.60      0.60      0.60         5\n",
            "       sugandeepak@gmail.com       0.50      1.00      0.67         1\n",
            "      sumathi_giri@gmail.com       0.50      0.33      0.40         6\n",
            "         sunny9999@gmail.com       0.00      0.00      0.00         1\n",
            "           test123@gmail.com       0.33      1.00      0.50         1\n",
            "           test1@uwindsor.ca       0.67      0.50      0.57         4\n",
            "             test2@gmail.com       0.00      0.00      0.00         7\n",
            "            test@uwindsor.ca       0.33      1.00      0.50         1\n",
            "     umaselvaraj70@gmail.com       0.00      0.00      0.00         1\n",
            "         vagdoda@uwindsor.ca       0.00      0.00      0.00         2\n",
            "           vijay@uwindsor.ca       0.50      0.50      0.50         2\n",
            "     vijayraghavan@gmail.com       0.00      0.00      0.00         2\n",
            "           zewbare@gmail.com       0.50      0.50      0.50         2\n",
            "\n",
            "                    accuracy                           0.38       273\n",
            "                   macro avg       0.32      0.35      0.30       273\n",
            "                weighted avg       0.40      0.38      0.36       273\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from ast import literal_eval\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/fake_data31July.csv')\n",
        "\n",
        "# Preprocessing steps\n",
        "def preprocess_data(data):\n",
        "    # Extract features from 'Hold Times' column\n",
        "    data['Hold Times'] = data['Hold Times'].apply(literal_eval)\n",
        "    data['Hold Times_Max'] = data['Hold Times'].apply(lambda x: np.max([float(val) for val in x]))\n",
        "    data['Hold Times_Min'] = data['Hold Times'].apply(lambda x: np.min([float(val) for val in x]))\n",
        "    data['Hold Times_Mean'] = data['Hold Times'].apply(lambda x: np.mean([float(val) for val in x]))\n",
        "\n",
        "    # Extract features from 'Flight Times' column\n",
        "    data['Flight Times'] = data['Flight Times'].apply(literal_eval)\n",
        "    data['Flight Times_Max'] = data['Flight Times'].apply(lambda x: np.max([float(val) for val in x]))\n",
        "    data['Flight Times_Min'] = data['Flight Times'].apply(lambda x: np.min([float(val) for val in x]))\n",
        "    data['Flight Times_Mean'] = data['Flight Times'].apply(lambda x: np.mean([float(val) for val in x]))\n",
        "\n",
        "    # Extract features from 'Press/Release Timings' column\n",
        "    data['Press/Release Timings'] = data['Press/Release Timings'].apply(literal_eval)\n",
        "    data['Press/Release Timings_Max'] = data['Press/Release Timings'].apply(lambda x: np.max([float(val[0]) for val in x]))\n",
        "    data['Press/Release Timings_Min'] = data['Press/Release Timings'].apply(lambda x: np.min([float(val[0]) for val in x]))\n",
        "    data['Press/Release Timings_Mean'] = data['Press/Release Timings'].apply(lambda x: np.mean([float(val[0]) for val in x]))\n",
        "\n",
        "    # Extract features from 'Key Combinations' column\n",
        "    data['Key Combinations_Unique_Count'] = data['Key Combinations'].apply(lambda x: len(set(x)))\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    data = data.drop(['Password', 'Email', 'Hold Times', 'Flight Times', 'Press/Release Timings'], axis=1)\n",
        "\n",
        "    # Separate numerical and categorical columns\n",
        "    numerical_columns = data.select_dtypes(include=[np.number]).columns\n",
        "    categorical_columns = data.select_dtypes(include=[object]).columns\n",
        "\n",
        "    # Preprocess numerical columns\n",
        "    scaler = StandardScaler()\n",
        "    data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
        "\n",
        "    # One-hot encode categorical columns\n",
        "    data = pd.get_dummies(data, columns=categorical_columns)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply preprocessing to the data\n",
        "preprocessed_data = preprocess_data(data)\n",
        "# Custom data augmentation function for tabular data\n",
        "def data_augmentation(X_train, y_train, num_augmentations=5, noise_factor=0.01):\n",
        "    X_train_augmented = []\n",
        "    y_train_augmented = []\n",
        "\n",
        "    for i in range(num_augmentations):\n",
        "        # Add noise to the training data\n",
        "        noise = np.random.normal(loc=0, scale=noise_factor, size=X_train.shape)\n",
        "        X_train_augmented.append(X_train + noise)\n",
        "        y_train_augmented.append(y_train)\n",
        "\n",
        "    # Concatenate the augmented data along the first axis to form one array\n",
        "    X_train_augmented = np.concatenate(X_train_augmented, axis=0)\n",
        "    y_train_augmented = np.concatenate(y_train_augmented, axis=0)\n",
        "\n",
        "    return X_train_augmented, y_train_augmented\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = preprocessed_data.drop('ID', axis=1).values\n",
        "y = preprocessed_data['ID']\n",
        "y_encoded = pd.get_dummies(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply data augmentation to the training data\n",
        "num_augmentations = 5\n",
        "noise_factor = 0.01\n",
        "X_train_augmented, y_train_augmented = data_augmentation(X_train, y_train, num_augmentations, noise_factor)\n",
        "\n",
        "\n",
        "# Models and evaluating it\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models, backend as K\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Convert the preprocessed data to numpy arrays\n",
        "X = preprocessed_data.drop('ID', axis=1).values\n",
        "y = preprocessed_data['ID']\n",
        "y_encoded = pd.get_dummies(y)\n",
        "\n",
        "# Split the data into training and testing sets with correct target labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply data augmentation to the training data (step 2)\n",
        "num_augmentations = 5\n",
        "noise_factor = 0.01\n",
        "X_train_augmented, y_train_augmented = data_augmentation(X_train, y_train, num_augmentations, noise_factor)\n",
        "\n",
        "\n",
        "\n",
        "# Define Siamese CNN model\n",
        "input_shape = (X_train.shape[1],)\n",
        "input_left = layers.Input(shape=input_shape)\n",
        "input_right = layers.Input(shape=input_shape)\n",
        "\n",
        "siamese_cnn = models.Sequential([\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "])\n",
        "\n",
        "encoded_left = siamese_cnn(input_left)\n",
        "encoded_right = siamese_cnn(input_right)\n",
        "\n",
        "# Calculate the Euclidean distance between the two encoded inputs\n",
        "distance = layers.Subtract()([encoded_left, encoded_right])\n",
        "distance = layers.Lambda(lambda x: K.abs(x))(distance)  # Corrected line for calculating the absolute value\n",
        "\n",
        "# Output layer\n",
        "num_classes = len(y_encoded.columns)\n",
        "output = layers.Dense(num_classes, activation='softmax')(distance)\n",
        "\n",
        "# Siamese model\n",
        "siamese_model = models.Model(inputs=[input_left, input_right], outputs=output)\n",
        "\n",
        "# Compile and train Siamese CNN model\n",
        "siamese_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "siamese_model.fit([X_train_augmented, X_train_augmented], y_train_augmented, epochs=10, batch_size=32, validation_data=([X_test, X_test], y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = siamese_model.evaluate([X_test, X_test], y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "3EpJjKrUk_Ya"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}